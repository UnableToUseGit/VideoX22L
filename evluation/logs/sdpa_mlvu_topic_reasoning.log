2024-11-28 11:50:54.419 | DEBUG    | __main__:<module>:154 - This is a DEBUG message
2024-11-28 11:50:54.419 | INFO     | __main__:<module>:155 - This is an INFO message
########## /share/junjie/shuyan/VideoXL_weight_8
You are using a model of type qwen2 to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
_attn_implementation: sdpa
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|                                                            | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|                                       | 1/4 [00:06<00:18,  6.16s/it]Loading checkpoint shards:  50%|                          | 2/4 [00:08<00:07,  3.88s/it]Loading checkpoint shards:  75%|             | 3/4 [00:10<00:03,  3.18s/it]Loading checkpoint shards: 100%|| 4/4 [00:12<00:00,  2.68s/it]Loading checkpoint shards: 100%|| 4/4 [00:12<00:00,  3.18s/it]
 reload 
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
  0%|                                                                                     | 0/264 [00:00<?, ?it/s]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the main background of the video?
Options:
(A) Grassland
(B) Lake
(C) Ocean
(D) Desert
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
2024-11-28 11:51:35.199 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:35.199 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:51:35.199 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:51:35.199 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:51:35.199 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:51:35.228 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:51:35.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:35.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:51:35.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:51:35.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:51:35.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2194
2024-11-28 11:51:35.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 2195])
2024-11-28 11:51:35.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:51:35.595 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:35.595 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:35.595 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:35.595 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 755, 128])
2024-11-28 11:51:35.595 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2374
2024-11-28 11:51:35.595 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2375])
2024-11-28 11:51:35.815 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:35.869 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:35.869 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:51:35.869 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:51:35.869 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 935, 128])
2024-11-28 11:51:35.869 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2734
2024-11-28 11:51:35.869 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2735])
2024-11-28 11:51:36.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:51:36.161 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:36.161 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:51:36.161 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:51:36.161 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1295, 128])
2024-11-28 11:51:36.162 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3094
2024-11-28 11:51:36.162 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3095])
2024-11-28 11:51:36.416 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:51:36.461 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:36.461 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:51:36.461 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:51:36.461 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1655, 128])
2024-11-28 11:51:36.462 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3454
2024-11-28 11:51:36.462 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3455])
2024-11-28 11:51:36.722 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:51:36.767 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:36.768 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:36.768 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:36.768 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2015, 128])
2024-11-28 11:51:36.768 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3634
2024-11-28 11:51:36.768 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3635])
2024-11-28 11:51:37.008 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:37.049 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:37.049 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:51:37.049 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:51:37.049 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2195, 128])
2024-11-28 11:51:37.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4354
2024-11-28 11:51:37.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4355])
2024-11-28 11:51:37.378 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:51:37.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:37.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:37.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:37.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2915, 128])
2024-11-28 11:51:37.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:51:37.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4535])
2024-11-28 11:51:37.688 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:37.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:37.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:51:37.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:51:37.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:51:37.733 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5254
2024-11-28 11:51:37.733 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5255])
2024-11-28 11:51:38.078 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:51:38.130 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:38.130 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:51:38.131 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:51:38.131 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3815, 128])
2024-11-28 11:51:38.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5974
2024-11-28 11:51:38.135 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5975])
2024-11-28 11:51:38.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:51:38.548 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:38.548 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:51:38.548 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:51:38.549 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4535, 128])
2024-11-28 11:51:38.552 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6334
2024-11-28 11:51:38.552 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6335])
2024-11-28 11:51:38.869 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:51:38.916 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:38.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:38.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:38.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4895, 128])
2024-11-28 11:51:38.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6514
2024-11-28 11:51:38.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6515])
2024-11-28 11:51:39.209 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:39.252 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:39.252 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:39.252 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:39.252 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5075, 128])
2024-11-28 11:51:39.255 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6694
2024-11-28 11:51:39.255 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6695])
2024-11-28 11:51:39.548 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:39.591 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:39.591 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:39.591 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:39.591 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5255, 128])
2024-11-28 11:51:39.594 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6874
2024-11-28 11:51:39.594 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6875])
2024-11-28 11:51:39.890 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:39.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:39.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:39.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:39.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5435, 128])
2024-11-28 11:51:39.937 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7054
2024-11-28 11:51:39.937 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 7055])
2024-11-28 11:51:40.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:40.278 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:40.278 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:40.278 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:40.278 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5615, 128])
2024-11-28 11:51:40.282 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7234
2024-11-28 11:51:40.282 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 7235])
2024-11-28 11:51:40.585 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:40.628 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:40.628 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:51:40.628 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:51:40.628 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5795, 128])
2024-11-28 11:51:40.633 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7594
2024-11-28 11:51:40.633 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 7595])
2024-11-28 11:51:40.972 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:51:41.021 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:41.021 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:51:41.021 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:51:41.021 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6155, 128])
2024-11-28 11:51:41.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8314
2024-11-28 11:51:41.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8315])
2024-11-28 11:51:41.438 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:51:41.493 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:41.493 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:41.493 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:41.493 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6875, 128])
2024-11-28 11:51:41.498 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8494
2024-11-28 11:51:41.498 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8495])
2024-11-28 11:51:41.821 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:41.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:41.866 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:51:41.866 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:51:41.866 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7055, 128])
2024-11-28 11:51:41.872 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8854
2024-11-28 11:51:41.872 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 8855])
2024-11-28 11:51:42.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:51:42.284 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:42.284 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:51:42.284 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:51:42.284 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7415, 128])
2024-11-28 11:51:42.291 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9214
2024-11-28 11:51:42.291 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9215])
2024-11-28 11:51:42.660 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:51:42.710 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:42.710 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:51:42.710 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:51:42.710 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7775, 128])
2024-11-28 11:51:42.717 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:51:42.718 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9575])
2024-11-28 11:51:43.092 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:51:43.142 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:43.142 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:51:43.142 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:51:43.142 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:51:43.152 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10294
2024-11-28 11:51:43.153 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10295])
2024-11-28 11:51:43.605 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:51:43.662 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:43.662 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:43.662 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:43.662 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8855, 128])
2024-11-28 11:51:43.669 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10474
2024-11-28 11:51:43.669 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10475])
2024-11-28 11:51:44.025 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:44.071 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:44.071 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:51:44.071 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:51:44.071 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9035, 128])
2024-11-28 11:51:44.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10654
2024-11-28 11:51:44.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10655])
2024-11-28 11:51:44.438 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:51:44.484 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:44.484 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1080])
2024-11-28 11:51:44.484 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 216
2024-11-28 11:51:44.484 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9215, 128])
2024-11-28 11:51:44.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10294
2024-11-28 11:51:44.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1080, 10295])
2024-11-28 11:51:44.742 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1080, 128])
2024-11-28 11:51:44.776 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:44.776 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 46])
2024-11-28 11:51:44.776 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:51:44.776 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9431, 128])
2024-11-28 11:51:44.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9476
2024-11-28 11:51:44.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 46, 9477])
2024-11-28 11:51:44.823 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 46, 128])
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
2024-11-28 11:51:44.831 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:44.831 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:51:44.831 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:51:44.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9477, 128])
2024-11-28 11:51:44.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9477
2024-11-28 11:51:44.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9478])
2024-11-28 11:51:44.874 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:51:44.893 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:44.893 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:51:44.893 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:51:44.893 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9478, 128])
2024-11-28 11:51:44.893 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9478
2024-11-28 11:51:44.894 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9479])
2024-11-28 11:51:44.935 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:51:44.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:44.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:51:44.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:51:44.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9479, 128])
2024-11-28 11:51:44.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9479
2024-11-28 11:51:44.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9480])
2024-11-28 11:51:44.984 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:51:44.991 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:51:44.991 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:51:44.991 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:51:44.991 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9480, 128])
2024-11-28 11:51:44.991 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9480
2024-11-28 11:51:44.991 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9481])
2024-11-28 11:51:45.033 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (A) Grassland
Pred A) Grassland
##########
2222222 A A
11111111111111 A A
Part  Acc: 100.00%
------------------------------ topic_reasoning ------------------------------
  0%|                                                                          | 1/264 [00:19<1:25:56, 19.61s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What color is the scarf worn by the woman in the video?
Options:
(A) Red
(B) Blue
(C) White
(D) Pink
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:52:00.501 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:00.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:52:00.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:00.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:52:00.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:52:00.527 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:52:00.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:00.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:00.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:00.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:52:00.534 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1834
2024-11-28 11:52:00.534 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 1835])
2024-11-28 11:52:00.765 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:00.809 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:00.809 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:00.809 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:00.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 395, 128])
2024-11-28 11:52:00.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2014
2024-11-28 11:52:00.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2015])
2024-11-28 11:52:01.023 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:01.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:01.063 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:01.063 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:01.063 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 575, 128])
2024-11-28 11:52:01.063 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2374
2024-11-28 11:52:01.063 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2375])
2024-11-28 11:52:01.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:01.348 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:01.348 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:01.348 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:01.348 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 935, 128])
2024-11-28 11:52:01.349 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2734
2024-11-28 11:52:01.349 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2735])
2024-11-28 11:52:01.596 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:01.640 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:01.641 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:01.641 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:01.641 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1295, 128])
2024-11-28 11:52:01.641 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3094
2024-11-28 11:52:01.641 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3095])
2024-11-28 11:52:01.894 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:01.939 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:01.940 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:01.940 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:01.940 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1655, 128])
2024-11-28 11:52:01.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3814
2024-11-28 11:52:01.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3815])
2024-11-28 11:52:02.255 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:02.307 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:02.307 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:02.307 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:02.307 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2375, 128])
2024-11-28 11:52:02.309 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:52:02.309 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4535])
2024-11-28 11:52:02.639 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:02.691 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:02.691 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:02.691 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:02.691 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:52:02.693 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4894
2024-11-28 11:52:02.693 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4895])
2024-11-28 11:52:02.982 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:03.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:03.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:03.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:03.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3455, 128])
2024-11-28 11:52:03.030 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5074
2024-11-28 11:52:03.030 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 5075])
2024-11-28 11:52:03.295 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:03.337 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:03.337 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:03.337 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:03.337 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3635, 128])
2024-11-28 11:52:03.341 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5794
2024-11-28 11:52:03.341 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5795])
2024-11-28 11:52:03.697 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:03.750 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:03.751 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:03.751 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:03.751 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4355, 128])
2024-11-28 11:52:03.753 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5974
2024-11-28 11:52:03.753 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 5975])
2024-11-28 11:52:04.034 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:04.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:04.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:04.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:04.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4535, 128])
2024-11-28 11:52:04.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6154
2024-11-28 11:52:04.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6155])
2024-11-28 11:52:04.364 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:04.407 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:04.407 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:04.407 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:04.407 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4715, 128])
2024-11-28 11:52:04.412 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6874
2024-11-28 11:52:04.412 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 6875])
2024-11-28 11:52:04.791 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:04.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:04.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:04.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:04.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5435, 128])
2024-11-28 11:52:04.849 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7234
2024-11-28 11:52:04.849 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 7235])
2024-11-28 11:52:05.181 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:05.231 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:05.231 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:05.231 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:05.231 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5795, 128])
2024-11-28 11:52:05.237 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7954
2024-11-28 11:52:05.238 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7955])
2024-11-28 11:52:05.642 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:05.696 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:05.697 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:05.697 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:05.697 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6515, 128])
2024-11-28 11:52:05.701 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8134
2024-11-28 11:52:05.701 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8135])
2024-11-28 11:52:06.018 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:06.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:06.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:06.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:06.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6695, 128])
2024-11-28 11:52:06.067 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8314
2024-11-28 11:52:06.067 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8315])
2024-11-28 11:52:06.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:06.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:06.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:06.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:06.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6875, 128])
2024-11-28 11:52:06.437 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8674
2024-11-28 11:52:06.437 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 8675])
2024-11-28 11:52:06.796 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:06.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:06.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:06.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:06.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7235, 128])
2024-11-28 11:52:06.854 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9394
2024-11-28 11:52:06.854 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9395])
2024-11-28 11:52:07.288 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:07.344 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:07.344 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:07.345 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:07.345 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7955, 128])
2024-11-28 11:52:07.350 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:52:07.351 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9575])
2024-11-28 11:52:07.693 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:07.738 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:07.739 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:07.739 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:07.739 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:52:07.746 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9934
2024-11-28 11:52:07.746 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9935])
2024-11-28 11:52:08.128 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:08.178 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:08.178 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:08.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:08.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8495, 128])
2024-11-28 11:52:08.185 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10114
2024-11-28 11:52:08.185 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10115])
2024-11-28 11:52:08.536 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:08.581 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:08.581 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:08.581 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:08.581 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8675, 128])
2024-11-28 11:52:08.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10294
2024-11-28 11:52:08.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10295])
2024-11-28 11:52:08.941 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:08.987 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:08.987 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:08.987 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:08.987 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8855, 128])
2024-11-28 11:52:08.994 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10474
2024-11-28 11:52:08.995 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10475])
2024-11-28 11:52:09.350 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:09.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:09.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:09.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:09.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9035, 128])
2024-11-28 11:52:09.403 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10654
2024-11-28 11:52:09.404 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10655])
2024-11-28 11:52:09.764 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:09.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:09.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1296])
2024-11-28 11:52:09.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 432
2024-11-28 11:52:09.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9215, 128])
2024-11-28 11:52:09.815 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10510
2024-11-28 11:52:09.815 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1296, 10511])
2024-11-28 11:52:10.111 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1296, 128])
2024-11-28 11:52:10.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:10.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 49])
2024-11-28 11:52:10.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:10.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9647, 128])
2024-11-28 11:52:10.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9695
2024-11-28 11:52:10.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 49, 9696])
2024-11-28 11:52:10.199 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 49, 128])
2024-11-28 11:52:10.207 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:10.207 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:10.207 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:10.207 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9696, 128])
2024-11-28 11:52:10.207 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9696
2024-11-28 11:52:10.207 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9697])
2024-11-28 11:52:10.250 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:52:10.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:10.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:10.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:10.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9697, 128])
2024-11-28 11:52:10.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9697
2024-11-28 11:52:10.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9698])
2024-11-28 11:52:10.299 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:52:10.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:10.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:10.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:10.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9698, 128])
2024-11-28 11:52:10.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9698
2024-11-28 11:52:10.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9699])
2024-11-28 11:52:10.349 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (B) Blue
Pred B) Blue
##########
2222222 B B
11111111111111 B B
Part  Acc: 100.00%
------------------------------ topic_reasoning ------------------------------
  1%|                                                                          | 2/264 [00:44<1:40:16, 22.96s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What type of film is this?
Options:
(A) Mystery
(B) Comedy
(C) Romance
(D) Action
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:52:25.291 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:25.291 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:52:25.291 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:25.291 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:52:25.292 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:52:25.316 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:52:25.323 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:25.323 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:25.323 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:25.323 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:52:25.323 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1834
2024-11-28 11:52:25.323 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 1835])
2024-11-28 11:52:25.555 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:25.599 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:25.599 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:25.599 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:25.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 395, 128])
2024-11-28 11:52:25.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2194
2024-11-28 11:52:25.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2195])
2024-11-28 11:52:25.836 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:25.880 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:25.880 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:25.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:25.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 755, 128])
2024-11-28 11:52:25.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2554
2024-11-28 11:52:25.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2555])
2024-11-28 11:52:26.129 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:26.173 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:26.174 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:26.174 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:26.174 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1115, 128])
2024-11-28 11:52:26.175 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3274
2024-11-28 11:52:26.175 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3275])
2024-11-28 11:52:26.477 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:26.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:26.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:26.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:26.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1835, 128])
2024-11-28 11:52:26.529 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3634
2024-11-28 11:52:26.529 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3635])
2024-11-28 11:52:26.792 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:26.837 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:26.838 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:26.838 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:26.838 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2195, 128])
2024-11-28 11:52:26.840 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4354
2024-11-28 11:52:26.840 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4355])
2024-11-28 11:52:27.166 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:27.218 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:27.218 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:27.218 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:27.218 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2915, 128])
2024-11-28 11:52:27.219 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:52:27.219 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4535])
2024-11-28 11:52:27.474 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:27.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:27.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:27.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:27.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:52:27.517 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4714
2024-11-28 11:52:27.517 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4715])
2024-11-28 11:52:27.775 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:27.817 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:27.817 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:27.817 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:27.817 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3275, 128])
2024-11-28 11:52:27.819 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4894
2024-11-28 11:52:27.819 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4895])
2024-11-28 11:52:28.080 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:28.121 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:28.122 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:28.122 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:28.122 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3455, 128])
2024-11-28 11:52:28.123 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5074
2024-11-28 11:52:28.123 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 5075])
2024-11-28 11:52:28.388 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:28.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:28.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:28.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:28.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3635, 128])
2024-11-28 11:52:28.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5434
2024-11-28 11:52:28.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5435])
2024-11-28 11:52:28.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:28.776 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:28.776 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:28.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:28.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3995, 128])
2024-11-28 11:52:28.779 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5614
2024-11-28 11:52:28.779 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 5615])
2024-11-28 11:52:29.053 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:29.095 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:29.095 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:29.095 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:29.095 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4175, 128])
2024-11-28 11:52:29.098 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5974
2024-11-28 11:52:29.098 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5975])
2024-11-28 11:52:29.407 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:29.455 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:29.455 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:29.455 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:29.455 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4535, 128])
2024-11-28 11:52:29.460 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6694
2024-11-28 11:52:29.460 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 6695])
2024-11-28 11:52:29.836 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:29.891 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:29.891 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:29.891 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:29.891 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5255, 128])
2024-11-28 11:52:29.894 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6874
2024-11-28 11:52:29.894 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6875])
2024-11-28 11:52:30.190 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:30.233 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:30.233 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:30.233 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:30.233 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5435, 128])
2024-11-28 11:52:30.237 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7234
2024-11-28 11:52:30.237 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 7235])
2024-11-28 11:52:30.569 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:30.617 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:30.617 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:30.617 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:30.617 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5795, 128])
2024-11-28 11:52:30.624 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7954
2024-11-28 11:52:30.624 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7955])
2024-11-28 11:52:31.029 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:31.084 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:31.085 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:31.085 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:31.085 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6515, 128])
2024-11-28 11:52:31.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8314
2024-11-28 11:52:31.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 8315])
2024-11-28 11:52:31.441 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:31.490 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:31.490 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:31.490 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:31.490 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6875, 128])
2024-11-28 11:52:31.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8494
2024-11-28 11:52:31.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8495])
2024-11-28 11:52:31.817 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:31.862 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:31.862 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:31.862 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:31.862 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7055, 128])
2024-11-28 11:52:31.871 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9214
2024-11-28 11:52:31.871 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9215])
2024-11-28 11:52:32.299 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:32.355 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:32.355 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:32.355 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:32.355 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7775, 128])
2024-11-28 11:52:32.362 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:52:32.362 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9575])
2024-11-28 11:52:32.736 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:32.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:32.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:32.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:32.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:52:32.797 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10294
2024-11-28 11:52:32.797 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10295])
2024-11-28 11:52:33.249 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:33.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:33.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:33.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:33.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8855, 128])
2024-11-28 11:52:33.313 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10474
2024-11-28 11:52:33.313 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10475])
2024-11-28 11:52:33.669 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:33.714 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:33.715 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:33.715 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:33.715 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9035, 128])
2024-11-28 11:52:33.724 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10834
2024-11-28 11:52:33.724 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 10835])
2024-11-28 11:52:34.121 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:34.172 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:34.173 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:34.173 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:34.173 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9395, 128])
2024-11-28 11:52:34.185 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11554
2024-11-28 11:52:34.185 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 11555])
2024-11-28 11:52:34.665 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:34.723 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:34.724 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1296])
2024-11-28 11:52:34.724 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 432
2024-11-28 11:52:34.724 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10115, 128])
2024-11-28 11:52:34.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11410
2024-11-28 11:52:34.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1296, 11411])
2024-11-28 11:52:35.039 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1296, 128])
2024-11-28 11:52:35.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:35.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 43])
2024-11-28 11:52:35.080 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:35.080 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10547, 128])
2024-11-28 11:52:35.080 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10589
2024-11-28 11:52:35.080 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 43, 10590])
2024-11-28 11:52:35.129 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 43, 128])
2024-11-28 11:52:35.137 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:35.137 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:35.137 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:35.137 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10590, 128])
2024-11-28 11:52:35.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10590
2024-11-28 11:52:35.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10591])
2024-11-28 11:52:35.182 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:52:35.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:35.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:35.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:35.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10591, 128])
2024-11-28 11:52:35.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10591
2024-11-28 11:52:35.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10592])
2024-11-28 11:52:35.233 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:52:35.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:35.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:35.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:35.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10592, 128])
2024-11-28 11:52:35.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10592
2024-11-28 11:52:35.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10593])
2024-11-28 11:52:35.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (C) Romance
Pred C) Romance
##########
2222222 C C
11111111111111 C C
Part  Acc: 100.00%
------------------------------ topic_reasoning ------------------------------
  1%|                                                                          | 3/264 [01:09<1:43:50, 23.87s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What type of film is this?
Options:
(A) History
(B) Romance
(C) Action
(D) Comedy
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:52:45.084 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:45.085 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:52:45.085 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:45.085 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:52:45.085 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:52:45.109 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:52:45.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:45.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:45.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:45.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:52:45.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1834
2024-11-28 11:52:45.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 1835])
2024-11-28 11:52:45.347 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:45.391 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:45.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:45.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:45.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 395, 128])
2024-11-28 11:52:45.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2014
2024-11-28 11:52:45.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2015])
2024-11-28 11:52:45.605 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:45.645 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:45.645 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:45.645 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:45.645 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 575, 128])
2024-11-28 11:52:45.645 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2194
2024-11-28 11:52:45.646 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2195])
2024-11-28 11:52:45.861 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:45.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:45.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:45.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:45.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 755, 128])
2024-11-28 11:52:45.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2554
2024-11-28 11:52:45.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2555])
2024-11-28 11:52:46.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:46.188 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:46.188 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:46.188 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:46.188 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1115, 128])
2024-11-28 11:52:46.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3274
2024-11-28 11:52:46.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3275])
2024-11-28 11:52:46.491 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:46.542 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:46.542 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:46.542 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:46.542 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1835, 128])
2024-11-28 11:52:46.542 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3454
2024-11-28 11:52:46.542 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3455])
2024-11-28 11:52:46.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:46.818 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:46.818 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:46.818 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:46.818 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2015, 128])
2024-11-28 11:52:46.819 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3814
2024-11-28 11:52:46.820 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3815])
2024-11-28 11:52:47.086 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:47.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:47.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:47.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:47.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2375, 128])
2024-11-28 11:52:47.133 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3994
2024-11-28 11:52:47.133 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3995])
2024-11-28 11:52:47.380 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:47.422 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:47.422 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:47.422 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:47.422 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2555, 128])
2024-11-28 11:52:47.424 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4714
2024-11-28 11:52:47.425 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4715])
2024-11-28 11:52:47.757 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:47.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:47.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:47.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:47.810 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3275, 128])
2024-11-28 11:52:47.814 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5434
2024-11-28 11:52:47.814 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5435])
2024-11-28 11:52:48.161 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:48.214 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:48.214 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:48.214 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:48.214 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3995, 128])
2024-11-28 11:52:48.216 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5614
2024-11-28 11:52:48.216 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 5615])
2024-11-28 11:52:48.490 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:48.532 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:48.532 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:48.532 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:48.532 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4175, 128])
2024-11-28 11:52:48.534 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5794
2024-11-28 11:52:48.534 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 5795])
2024-11-28 11:52:48.812 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:48.853 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:48.854 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:48.854 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:48.854 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4355, 128])
2024-11-28 11:52:48.857 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6154
2024-11-28 11:52:48.857 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6155])
2024-11-28 11:52:49.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:49.215 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:49.216 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:49.216 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:49.216 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4715, 128])
2024-11-28 11:52:49.218 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6334
2024-11-28 11:52:49.219 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6335])
2024-11-28 11:52:49.504 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:49.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:49.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:49.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:49.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4895, 128])
2024-11-28 11:52:49.550 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6514
2024-11-28 11:52:49.550 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6515])
2024-11-28 11:52:49.838 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:49.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:49.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:49.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:49.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5075, 128])
2024-11-28 11:52:49.885 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6874
2024-11-28 11:52:49.885 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6875])
2024-11-28 11:52:50.209 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:50.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:50.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:50.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:50.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5435, 128])
2024-11-28 11:52:50.261 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7054
2024-11-28 11:52:50.261 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 7055])
2024-11-28 11:52:50.559 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:50.602 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:50.602 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:50.603 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:50.603 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5615, 128])
2024-11-28 11:52:50.609 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7774
2024-11-28 11:52:50.609 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7775])
2024-11-28 11:52:51.008 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:51.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:51.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:51.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:51.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6335, 128])
2024-11-28 11:52:51.070 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8494
2024-11-28 11:52:51.070 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8495])
2024-11-28 11:52:51.484 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:51.540 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:51.541 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:51.541 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:51.541 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7055, 128])
2024-11-28 11:52:51.545 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8674
2024-11-28 11:52:51.545 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8675])
2024-11-28 11:52:51.870 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:51.915 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:51.915 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:52:51.915 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:52:51.915 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7235, 128])
2024-11-28 11:52:51.921 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9034
2024-11-28 11:52:51.921 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9035])
2024-11-28 11:52:52.286 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:52:52.335 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:52.336 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:52.336 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:52.336 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7595, 128])
2024-11-28 11:52:52.345 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9754
2024-11-28 11:52:52.345 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9755])
2024-11-28 11:52:52.787 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:52.844 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:52.844 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:52.844 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:52.844 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8315, 128])
2024-11-28 11:52:52.851 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9934
2024-11-28 11:52:52.851 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9935])
2024-11-28 11:52:53.197 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:53.242 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:53.242 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:52:53.242 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:52:53.242 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8495, 128])
2024-11-28 11:52:53.253 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10654
2024-11-28 11:52:53.253 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10655])
2024-11-28 11:52:53.713 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:52:53.770 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:53.770 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:52:53.770 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:52:53.771 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9215, 128])
2024-11-28 11:52:53.778 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10834
2024-11-28 11:52:53.778 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10835])
2024-11-28 11:52:54.139 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:52:54.185 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:54.185 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1296])
2024-11-28 11:52:54.185 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 432
2024-11-28 11:52:54.185 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9395, 128])
2024-11-28 11:52:54.191 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10690
2024-11-28 11:52:54.191 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1296, 10691])
2024-11-28 11:52:54.491 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1296, 128])
2024-11-28 11:52:54.531 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:54.531 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 43])
2024-11-28 11:52:54.531 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:54.531 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9827, 128])
2024-11-28 11:52:54.531 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9869
2024-11-28 11:52:54.531 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 43, 9870])
2024-11-28 11:52:54.579 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 43, 128])
2024-11-28 11:52:54.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:54.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:54.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:54.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9870, 128])
2024-11-28 11:52:54.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9870
2024-11-28 11:52:54.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9871])
2024-11-28 11:52:54.631 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:52:54.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:54.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:54.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:54.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9871, 128])
2024-11-28 11:52:54.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9871
2024-11-28 11:52:54.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9872])
2024-11-28 11:52:54.681 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:52:54.688 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:52:54.688 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:52:54.688 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:52:54.688 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9872, 128])
2024-11-28 11:52:54.688 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9872
2024-11-28 11:52:54.688 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 9873])
2024-11-28 11:52:54.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (C) Action
Pred B) Romance
##########
2222222 B C
Part  Acc: 75.00%
------------------------------ topic_reasoning ------------------------------
  2%|                                                                         | 4/264 [01:29<1:35:50, 22.12s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the genre of this film?
Options:
(A) Sci-Fi
(B) Romance
(C) Action
(D) Mystery
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:53:08.363 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:08.363 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:53:08.363 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:08.363 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:53:08.364 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:53:08.389 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:53:08.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:08.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:08.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:08.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:53:08.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1654
2024-11-28 11:53:08.396 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 1655])
2024-11-28 11:53:08.604 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:08.643 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:08.644 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:08.644 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:08.644 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 215, 128])
2024-11-28 11:53:08.644 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1834
2024-11-28 11:53:08.644 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 1835])
2024-11-28 11:53:08.856 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:08.896 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:08.896 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:08.896 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:08.896 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 395, 128])
2024-11-28 11:53:08.896 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2014
2024-11-28 11:53:08.896 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2015])
2024-11-28 11:53:09.111 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:09.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:09.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:09.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:09.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 575, 128])
2024-11-28 11:53:09.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2374
2024-11-28 11:53:09.151 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2375])
2024-11-28 11:53:09.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:09.437 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:09.437 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:09.437 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:09.437 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 935, 128])
2024-11-28 11:53:09.437 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2734
2024-11-28 11:53:09.437 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2735])
2024-11-28 11:53:09.685 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:09.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:09.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:09.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:09.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1295, 128])
2024-11-28 11:53:09.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2914
2024-11-28 11:53:09.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2915])
2024-11-28 11:53:09.959 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:09.999 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:09.999 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:09.999 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:09.999 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1475, 128])
2024-11-28 11:53:10.001 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3634
2024-11-28 11:53:10.001 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3635])
2024-11-28 11:53:10.313 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:10.365 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:10.365 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:10.365 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:10.365 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2195, 128])
2024-11-28 11:53:10.366 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3814
2024-11-28 11:53:10.366 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3815])
2024-11-28 11:53:10.611 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:10.652 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:10.652 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:10.652 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:10.652 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2375, 128])
2024-11-28 11:53:10.654 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4174
2024-11-28 11:53:10.654 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4175])
2024-11-28 11:53:10.930 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:10.977 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:10.977 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:10.977 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:10.977 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2735, 128])
2024-11-28 11:53:10.979 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:53:10.979 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4535])
2024-11-28 11:53:11.268 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:11.314 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:11.314 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:11.315 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:11.315 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:53:11.316 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4894
2024-11-28 11:53:11.316 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4895])
2024-11-28 11:53:11.608 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:11.655 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:11.655 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:11.655 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:11.655 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3455, 128])
2024-11-28 11:53:11.657 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5254
2024-11-28 11:53:11.657 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5255])
2024-11-28 11:53:11.955 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:12.003 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:12.003 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:12.003 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:12.003 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3815, 128])
2024-11-28 11:53:12.006 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5974
2024-11-28 11:53:12.006 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5975])
2024-11-28 11:53:12.369 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:12.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:12.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:12.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:12.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4535, 128])
2024-11-28 11:53:12.428 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6694
2024-11-28 11:53:12.428 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 6695])
2024-11-28 11:53:12.805 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:12.859 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:12.859 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:12.860 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:12.860 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5255, 128])
2024-11-28 11:53:12.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7414
2024-11-28 11:53:12.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7415])
2024-11-28 11:53:13.258 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:13.313 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:13.313 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:13.313 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:13.313 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5975, 128])
2024-11-28 11:53:13.320 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8134
2024-11-28 11:53:13.320 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8135])
2024-11-28 11:53:13.729 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:13.785 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:13.785 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:13.785 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:13.785 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6695, 128])
2024-11-28 11:53:13.791 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8494
2024-11-28 11:53:13.791 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 8495])
2024-11-28 11:53:14.147 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:14.196 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:14.197 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:14.197 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:14.197 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7055, 128])
2024-11-28 11:53:14.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8674
2024-11-28 11:53:14.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8675])
2024-11-28 11:53:14.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:14.573 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:14.573 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:14.573 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:14.573 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7235, 128])
2024-11-28 11:53:14.579 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9034
2024-11-28 11:53:14.580 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9035])
2024-11-28 11:53:14.945 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:14.995 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:14.995 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:14.995 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:14.995 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7595, 128])
2024-11-28 11:53:15.002 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9394
2024-11-28 11:53:15.002 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9395])
2024-11-28 11:53:15.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:15.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:15.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:15.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:15.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7955, 128])
2024-11-28 11:53:15.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:53:15.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9575])
2024-11-28 11:53:15.771 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:15.816 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:15.816 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:15.816 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:15.816 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:53:15.824 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9934
2024-11-28 11:53:15.824 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9935])
2024-11-28 11:53:16.206 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:16.256 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:16.256 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:16.256 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:16.256 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8495, 128])
2024-11-28 11:53:16.267 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10654
2024-11-28 11:53:16.267 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10655])
2024-11-28 11:53:16.729 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:16.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:16.787 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:16.787 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:16.787 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9215, 128])
2024-11-28 11:53:16.794 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10834
2024-11-28 11:53:16.795 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10835])
2024-11-28 11:53:17.158 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:17.203 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:17.204 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:17.204 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:17.204 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9395, 128])
2024-11-28 11:53:17.213 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11194
2024-11-28 11:53:17.213 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 11195])
2024-11-28 11:53:17.620 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:17.671 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:17.672 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1080])
2024-11-28 11:53:17.672 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 216
2024-11-28 11:53:17.672 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9755, 128])
2024-11-28 11:53:17.676 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10834
2024-11-28 11:53:17.676 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1080, 10835])
2024-11-28 11:53:17.938 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1080, 128])
2024-11-28 11:53:17.972 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:17.973 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 45])
2024-11-28 11:53:17.973 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:17.973 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9971, 128])
2024-11-28 11:53:17.973 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10015
2024-11-28 11:53:17.973 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 45, 10016])
2024-11-28 11:53:18.020 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 45, 128])
2024-11-28 11:53:18.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:18.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:53:18.029 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:18.029 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10016, 128])
2024-11-28 11:53:18.029 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10016
2024-11-28 11:53:18.029 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10017])
2024-11-28 11:53:18.072 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:53:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:53:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10017, 128])
2024-11-28 11:53:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10017
2024-11-28 11:53:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10018])
2024-11-28 11:53:18.122 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:53:18.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:18.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:53:18.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:18.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10018, 128])
2024-11-28 11:53:18.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10018
2024-11-28 11:53:18.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10019])
2024-11-28 11:53:18.186 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (C) Action
Pred C) Action
##########
2222222 C C
11111111111111 C C
Part  Acc: 80.00%
------------------------------ topic_reasoning ------------------------------
  2%|                                                                         | 5/264 [01:52<1:37:33, 22.60s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the first-person character doing in this video?
Options:
(A) Making coffee
(B) Making milk
(C) Making a cake
(D) Baking cookies
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:53:36.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:36.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:53:36.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:36.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:53:36.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:53:36.926 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:53:36.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:36.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:36.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:36.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:53:36.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1834
2024-11-28 11:53:36.933 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 1835])
2024-11-28 11:53:37.163 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:37.207 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:37.208 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:37.208 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:37.208 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 395, 128])
2024-11-28 11:53:37.208 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2194
2024-11-28 11:53:37.208 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2195])
2024-11-28 11:53:37.444 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:37.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:37.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:37.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:37.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 755, 128])
2024-11-28 11:53:37.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2914
2024-11-28 11:53:37.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 2915])
2024-11-28 11:53:37.781 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:37.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:37.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:37.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:37.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1475, 128])
2024-11-28 11:53:37.833 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3274
2024-11-28 11:53:37.833 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3275])
2024-11-28 11:53:38.088 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:38.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:38.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:38.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:38.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1835, 128])
2024-11-28 11:53:38.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3454
2024-11-28 11:53:38.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3455])
2024-11-28 11:53:38.369 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:38.410 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:38.410 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:38.410 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:38.410 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2015, 128])
2024-11-28 11:53:38.412 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4174
2024-11-28 11:53:38.412 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4175])
2024-11-28 11:53:38.734 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:38.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:38.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:38.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:38.786 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2735, 128])
2024-11-28 11:53:38.788 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:53:38.788 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4535])
2024-11-28 11:53:39.069 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:39.115 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:39.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:39.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:39.116 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:53:39.119 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5254
2024-11-28 11:53:39.119 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5255])
2024-11-28 11:53:39.462 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:39.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:39.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:39.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:39.515 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3815, 128])
2024-11-28 11:53:39.518 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5614
2024-11-28 11:53:39.518 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5615])
2024-11-28 11:53:39.818 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:39.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:39.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:39.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:39.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4175, 128])
2024-11-28 11:53:39.868 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5974
2024-11-28 11:53:39.868 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5975])
2024-11-28 11:53:40.176 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:40.222 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:40.223 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:40.223 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:40.223 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4535, 128])
2024-11-28 11:53:40.225 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6154
2024-11-28 11:53:40.225 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6155])
2024-11-28 11:53:40.509 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:40.551 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:40.551 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:40.551 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:40.552 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4715, 128])
2024-11-28 11:53:40.557 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6874
2024-11-28 11:53:40.557 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 6875])
2024-11-28 11:53:40.936 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:40.990 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:40.990 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:40.990 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:40.990 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5435, 128])
2024-11-28 11:53:40.996 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7594
2024-11-28 11:53:40.997 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7595])
2024-11-28 11:53:41.391 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:41.446 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:41.446 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:41.446 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:41.446 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6155, 128])
2024-11-28 11:53:41.451 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7954
2024-11-28 11:53:41.451 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 7955])
2024-11-28 11:53:41.796 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:41.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:41.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:41.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:41.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6515, 128])
2024-11-28 11:53:41.849 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8134
2024-11-28 11:53:41.849 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8135])
2024-11-28 11:53:42.166 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:42.210 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:42.210 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:42.210 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:42.210 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6695, 128])
2024-11-28 11:53:42.215 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8314
2024-11-28 11:53:42.215 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8315])
2024-11-28 11:53:42.534 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:42.579 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:42.579 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:42.579 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:42.579 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6875, 128])
2024-11-28 11:53:42.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9034
2024-11-28 11:53:42.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9035])
2024-11-28 11:53:43.011 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:43.067 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:43.067 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:43.067 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:43.067 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7595, 128])
2024-11-28 11:53:43.077 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9754
2024-11-28 11:53:43.077 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9755])
2024-11-28 11:53:43.518 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:43.575 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:43.575 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:43.575 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:43.575 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8315, 128])
2024-11-28 11:53:43.582 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9934
2024-11-28 11:53:43.582 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9935])
2024-11-28 11:53:43.928 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:43.973 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:43.974 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:43.974 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:43.974 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8495, 128])
2024-11-28 11:53:43.984 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10654
2024-11-28 11:53:43.985 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10655])
2024-11-28 11:53:44.444 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:44.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:44.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:44.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:44.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9215, 128])
2024-11-28 11:53:44.509 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10834
2024-11-28 11:53:44.510 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10835])
2024-11-28 11:53:44.871 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:44.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:44.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:44.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:44.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9395, 128])
2024-11-28 11:53:44.929 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11554
2024-11-28 11:53:44.929 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 11555])
2024-11-28 11:53:45.410 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:45.469 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:45.469 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:53:45.469 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:53:45.469 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10115, 128])
2024-11-28 11:53:45.478 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11734
2024-11-28 11:53:45.478 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 11735])
2024-11-28 11:53:45.856 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:53:45.903 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:45.904 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:53:45.904 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:53:45.904 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10295, 128])
2024-11-28 11:53:45.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12454
2024-11-28 11:53:45.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 12455])
2024-11-28 11:53:46.416 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:53:46.475 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:46.475 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:53:46.475 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:53:46.475 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11015, 128])
2024-11-28 11:53:46.487 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12814
2024-11-28 11:53:46.487 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 12815])
2024-11-28 11:53:46.923 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:53:46.976 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:46.976 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1080])
2024-11-28 11:53:46.976 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 216
2024-11-28 11:53:46.976 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11375, 128])
2024-11-28 11:53:46.981 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12454
2024-11-28 11:53:46.981 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1080, 12455])
2024-11-28 11:53:47.265 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1080, 128])
2024-11-28 11:53:47.301 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:47.301 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 53])
2024-11-28 11:53:47.301 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:47.301 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11591, 128])
2024-11-28 11:53:47.301 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11643
2024-11-28 11:53:47.301 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 53, 11644])
2024-11-28 11:53:47.355 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 53, 128])
2024-11-28 11:53:47.364 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:47.364 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:53:47.365 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:47.365 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11644, 128])
2024-11-28 11:53:47.365 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11644
2024-11-28 11:53:47.365 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11645])
2024-11-28 11:53:47.412 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:53:47.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:47.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:53:47.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:47.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11645, 128])
2024-11-28 11:53:47.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11645
2024-11-28 11:53:47.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11646])
2024-11-28 11:53:47.480 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:53:47.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:47.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:53:47.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:47.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11646, 128])
2024-11-28 11:53:47.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11646
2024-11-28 11:53:47.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11647])
2024-11-28 11:53:47.536 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:53:47.543 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:53:47.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:53:47.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:53:47.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11647, 128])
2024-11-28 11:53:47.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11647
2024-11-28 11:53:47.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11648])
2024-11-28 11:53:47.591 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (A) Making coffee
Pred A) Making coffee
##########
2222222 A A
11111111111111 A A
Part  Acc: 83.33%
------------------------------ topic_reasoning ------------------------------
  2%|                                                                         | 6/264 [02:22<1:47:07, 24.91s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What story does the whole video tell?
Options:
(A) Criminal Investigation
(B) Wedding Scene
(C) Drama Performance
(D) Chase Incident
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:54:00.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:00.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:54:00.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:00.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:54:00.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:54:00.416 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:54:00.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:00.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:00.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:00.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:54:00.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2194
2024-11-28 11:54:00.423 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 2195])
2024-11-28 11:54:00.702 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:00.751 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:00.752 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:00.752 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:00.752 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 755, 128])
2024-11-28 11:54:00.752 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2554
2024-11-28 11:54:00.752 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2555])
2024-11-28 11:54:00.994 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:01.039 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:01.039 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:01.039 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:01.039 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1115, 128])
2024-11-28 11:54:01.039 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2914
2024-11-28 11:54:01.039 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2915])
2024-11-28 11:54:01.288 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:01.333 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:01.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:01.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:01.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1475, 128])
2024-11-28 11:54:01.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3094
2024-11-28 11:54:01.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3095])
2024-11-28 11:54:01.564 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:01.604 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:01.604 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:01.604 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:01.604 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1655, 128])
2024-11-28 11:54:01.604 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3274
2024-11-28 11:54:01.605 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3275])
2024-11-28 11:54:01.837 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:01.878 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:01.878 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:01.878 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:01.878 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1835, 128])
2024-11-28 11:54:01.879 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3634
2024-11-28 11:54:01.879 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3635])
2024-11-28 11:54:02.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:02.188 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:02.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:02.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:02.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2195, 128])
2024-11-28 11:54:02.190 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3994
2024-11-28 11:54:02.190 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3995])
2024-11-28 11:54:02.462 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:02.508 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:02.508 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:02.508 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:02.508 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2555, 128])
2024-11-28 11:54:02.509 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4354
2024-11-28 11:54:02.509 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4355])
2024-11-28 11:54:02.787 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:02.833 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:02.833 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:02.834 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:02.834 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2915, 128])
2024-11-28 11:54:02.835 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4714
2024-11-28 11:54:02.835 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4715])
2024-11-28 11:54:03.120 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:03.166 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:03.167 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:03.167 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:03.167 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3275, 128])
2024-11-28 11:54:03.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4894
2024-11-28 11:54:03.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4895])
2024-11-28 11:54:03.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:03.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:03.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:03.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:03.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3455, 128])
2024-11-28 11:54:03.475 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5614
2024-11-28 11:54:03.475 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5615])
2024-11-28 11:54:03.827 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:03.880 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:03.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:03.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:03.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4175, 128])
2024-11-28 11:54:03.885 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6334
2024-11-28 11:54:03.885 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 6335])
2024-11-28 11:54:04.252 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:04.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:04.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:04.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:04.306 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4895, 128])
2024-11-28 11:54:04.310 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6694
2024-11-28 11:54:04.310 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6695])
2024-11-28 11:54:04.632 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:04.680 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:04.680 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:04.680 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:04.680 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5255, 128])
2024-11-28 11:54:04.686 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7414
2024-11-28 11:54:04.686 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7415])
2024-11-28 11:54:05.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:05.130 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:05.131 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:05.131 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:05.131 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5975, 128])
2024-11-28 11:54:05.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8134
2024-11-28 11:54:05.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8135])
2024-11-28 11:54:05.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:05.603 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:05.603 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:05.603 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:05.603 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6695, 128])
2024-11-28 11:54:05.611 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8854
2024-11-28 11:54:05.611 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8855])
2024-11-28 11:54:06.034 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:06.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:06.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:06.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:06.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7415, 128])
2024-11-28 11:54:06.096 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9034
2024-11-28 11:54:06.096 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9035])
2024-11-28 11:54:06.427 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:06.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:06.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:06.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:06.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7595, 128])
2024-11-28 11:54:06.478 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9214
2024-11-28 11:54:06.478 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9215])
2024-11-28 11:54:06.813 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:06.858 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:06.858 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:06.858 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:06.858 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7775, 128])
2024-11-28 11:54:06.864 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9394
2024-11-28 11:54:06.864 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9395])
2024-11-28 11:54:07.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:07.247 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:07.247 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:07.247 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:07.247 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7955, 128])
2024-11-28 11:54:07.254 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9754
2024-11-28 11:54:07.254 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9755])
2024-11-28 11:54:07.633 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:07.683 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:07.683 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:07.683 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:07.684 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8315, 128])
2024-11-28 11:54:07.690 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9934
2024-11-28 11:54:07.690 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9935])
2024-11-28 11:54:08.037 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:08.083 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:08.083 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:08.083 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:08.083 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8495, 128])
2024-11-28 11:54:08.094 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10654
2024-11-28 11:54:08.094 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10655])
2024-11-28 11:54:08.555 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:08.613 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:08.614 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:08.614 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:08.614 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9215, 128])
2024-11-28 11:54:08.621 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10834
2024-11-28 11:54:08.621 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10835])
2024-11-28 11:54:08.982 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:09.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:09.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:09.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:09.028 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9395, 128])
2024-11-28 11:54:09.037 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11194
2024-11-28 11:54:09.037 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 11195])
2024-11-28 11:54:09.441 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:09.494 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:09.494 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:09.494 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:09.494 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9755, 128])
2024-11-28 11:54:09.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11374
2024-11-28 11:54:09.502 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 11375])
2024-11-28 11:54:09.874 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:09.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:09.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1296])
2024-11-28 11:54:09.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 432
2024-11-28 11:54:09.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9935, 128])
2024-11-28 11:54:09.926 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11230
2024-11-28 11:54:09.926 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1296, 11231])
2024-11-28 11:54:10.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1296, 128])
2024-11-28 11:54:10.272 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:10.272 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 48])
2024-11-28 11:54:10.272 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:10.272 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10367, 128])
2024-11-28 11:54:10.272 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10414
2024-11-28 11:54:10.272 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 48, 10415])
2024-11-28 11:54:10.321 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 48, 128])
2024-11-28 11:54:10.329 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:10.330 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:10.330 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:10.330 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10415, 128])
2024-11-28 11:54:10.330 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10415
2024-11-28 11:54:10.330 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10416])
2024-11-28 11:54:10.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:54:10.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:10.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:10.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:10.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10416, 128])
2024-11-28 11:54:10.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10416
2024-11-28 11:54:10.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10417])
2024-11-28 11:54:10.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:54:10.442 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:10.443 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:10.443 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:10.443 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10417, 128])
2024-11-28 11:54:10.443 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10417
2024-11-28 11:54:10.443 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10418])
2024-11-28 11:54:10.487 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:54:10.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:10.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:10.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:10.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10418, 128])
2024-11-28 11:54:10.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10418
2024-11-28 11:54:10.495 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10419])
2024-11-28 11:54:10.539 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (A) Criminal Investigation
Pred A) Criminal Investigation
##########
2222222 A A
11111111111111 A A
Part  Acc: 85.71%
------------------------------ topic_reasoning ------------------------------
  3%|                                                                         | 7/264 [02:45<1:43:57, 24.27s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What season is it in the video?
Options:
(A) Summer
(B) Spring
(C) Autumn
(D) Winter
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:54:24.026 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:24.026 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:54:24.026 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:24.026 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:54:24.026 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:54:24.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:54:24.058 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:24.058 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:24.058 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:24.058 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:54:24.058 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1834
2024-11-28 11:54:24.058 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 1835])
2024-11-28 11:54:24.289 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:24.333 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:24.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:24.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:24.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 395, 128])
2024-11-28 11:54:24.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2014
2024-11-28 11:54:24.334 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2015])
2024-11-28 11:54:24.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:24.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:24.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:24.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:24.587 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 575, 128])
2024-11-28 11:54:24.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2374
2024-11-28 11:54:24.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2375])
2024-11-28 11:54:24.830 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:24.874 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:24.875 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:24.875 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:24.875 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 935, 128])
2024-11-28 11:54:24.875 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2734
2024-11-28 11:54:24.875 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2735])
2024-11-28 11:54:25.123 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:25.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:25.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:25.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:25.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1295, 128])
2024-11-28 11:54:25.170 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3454
2024-11-28 11:54:25.170 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3455])
2024-11-28 11:54:25.476 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:25.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:25.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:25.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:25.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2015, 128])
2024-11-28 11:54:25.529 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3814
2024-11-28 11:54:25.529 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3815])
2024-11-28 11:54:25.800 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:25.851 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:25.851 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:25.851 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:25.851 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2375, 128])
2024-11-28 11:54:25.851 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:54:25.852 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4535])
2024-11-28 11:54:26.182 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:26.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:26.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:26.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:26.235 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:54:26.238 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5254
2024-11-28 11:54:26.238 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5255])
2024-11-28 11:54:26.586 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:26.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:26.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:26.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:26.639 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3815, 128])
2024-11-28 11:54:26.640 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5434
2024-11-28 11:54:26.640 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 5435])
2024-11-28 11:54:26.912 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:26.954 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:26.954 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:26.954 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:26.954 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3995, 128])
2024-11-28 11:54:26.956 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5794
2024-11-28 11:54:26.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5795])
2024-11-28 11:54:27.265 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:27.312 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:27.312 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:27.312 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:27.312 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4355, 128])
2024-11-28 11:54:27.315 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6154
2024-11-28 11:54:27.315 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6155])
2024-11-28 11:54:27.629 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:27.675 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:27.676 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:27.676 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:27.676 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4715, 128])
2024-11-28 11:54:27.679 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6514
2024-11-28 11:54:27.679 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6515])
2024-11-28 11:54:27.998 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:28.045 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:28.046 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:28.046 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:28.046 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5075, 128])
2024-11-28 11:54:28.049 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6694
2024-11-28 11:54:28.049 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6695])
2024-11-28 11:54:28.343 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:28.385 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:28.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:28.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:28.386 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5255, 128])
2024-11-28 11:54:28.391 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7414
2024-11-28 11:54:28.391 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7415])
2024-11-28 11:54:28.785 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:28.840 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:28.840 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:28.840 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:28.840 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5975, 128])
2024-11-28 11:54:28.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7774
2024-11-28 11:54:28.845 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 7775])
2024-11-28 11:54:29.188 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:29.236 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:29.237 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:29.237 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:29.237 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6335, 128])
2024-11-28 11:54:29.244 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8494
2024-11-28 11:54:29.244 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8495])
2024-11-28 11:54:29.661 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:29.716 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:29.716 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:29.716 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:29.716 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7055, 128])
2024-11-28 11:54:29.723 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8854
2024-11-28 11:54:29.723 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 8855])
2024-11-28 11:54:30.085 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:30.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:30.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:30.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:30.135 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7415, 128])
2024-11-28 11:54:30.140 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9034
2024-11-28 11:54:30.140 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9035])
2024-11-28 11:54:30.472 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:30.517 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:30.517 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:30.517 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:30.517 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7595, 128])
2024-11-28 11:54:30.524 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9394
2024-11-28 11:54:30.524 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9395])
2024-11-28 11:54:30.895 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:30.945 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:30.945 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:30.945 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:30.945 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7955, 128])
2024-11-28 11:54:30.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:54:30.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9575])
2024-11-28 11:54:31.292 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:31.337 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:31.337 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:31.337 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:31.337 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:54:31.344 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9934
2024-11-28 11:54:31.344 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9935])
2024-11-28 11:54:31.726 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:31.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:31.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:31.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:31.777 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8495, 128])
2024-11-28 11:54:31.784 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10114
2024-11-28 11:54:31.784 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 10115])
2024-11-28 11:54:32.135 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:32.181 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:32.181 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:32.181 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:32.181 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8675, 128])
2024-11-28 11:54:32.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10474
2024-11-28 11:54:32.189 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 10475])
2024-11-28 11:54:32.581 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:32.632 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:32.632 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:32.632 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:32.632 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9035, 128])
2024-11-28 11:54:32.643 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11194
2024-11-28 11:54:32.643 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 11195])
2024-11-28 11:54:33.115 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:33.173 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:33.173 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:33.173 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:33.173 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9755, 128])
2024-11-28 11:54:33.182 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11374
2024-11-28 11:54:33.182 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 11375])
2024-11-28 11:54:33.553 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:33.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:33.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 972])
2024-11-28 11:54:33.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 108
2024-11-28 11:54:33.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9935, 128])
2024-11-28 11:54:33.604 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10906
2024-11-28 11:54:33.604 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 972, 10907])
2024-11-28 11:54:33.860 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 972, 128])
2024-11-28 11:54:33.892 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:33.892 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 44])
2024-11-28 11:54:33.892 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:33.892 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10043, 128])
2024-11-28 11:54:33.892 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10086
2024-11-28 11:54:33.893 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 44, 10087])
2024-11-28 11:54:33.942 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 44, 128])
2024-11-28 11:54:33.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:33.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:33.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:33.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10087, 128])
2024-11-28 11:54:33.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10087
2024-11-28 11:54:33.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10088])
2024-11-28 11:54:33.994 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:54:34.018 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:34.018 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:34.018 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:34.018 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10088, 128])
2024-11-28 11:54:34.018 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10088
2024-11-28 11:54:34.018 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10089])
2024-11-28 11:54:34.062 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:54:34.070 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:34.070 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:34.070 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:34.070 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10089, 128])
2024-11-28 11:54:34.070 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10089
2024-11-28 11:54:34.070 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10090])
2024-11-28 11:54:34.113 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (D) Winter
Pred D) Winter
##########
2222222 D D
11111111111111 D D
Part  Acc: 87.50%
------------------------------ topic_reasoning ------------------------------
  3%|                                                                        | 8/264 [03:08<1:42:36, 24.05s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the background of the video?
Options:
(A) Desert
(B) Undersea
(C) Forest
(D) Beach
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:54:40.077 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:40.077 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:54:40.077 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:40.077 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:54:40.077 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:54:40.102 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:54:40.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:40.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:40.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:40.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:54:40.109 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2194
2024-11-28 11:54:40.109 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 2195])
2024-11-28 11:54:40.389 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:40.440 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:40.440 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:40.440 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:40.440 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 755, 128])
2024-11-28 11:54:40.440 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2374
2024-11-28 11:54:40.440 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2375])
2024-11-28 11:54:40.658 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:40.698 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:40.698 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:40.698 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:40.698 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 935, 128])
2024-11-28 11:54:40.699 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3094
2024-11-28 11:54:40.699 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3095])
2024-11-28 11:54:40.996 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:41.047 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:41.048 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:41.048 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:41.048 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1655, 128])
2024-11-28 11:54:41.048 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3274
2024-11-28 11:54:41.048 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3275])
2024-11-28 11:54:41.281 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:41.321 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:41.322 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:41.322 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:41.322 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1835, 128])
2024-11-28 11:54:41.322 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3454
2024-11-28 11:54:41.322 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3455])
2024-11-28 11:54:41.558 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:41.599 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:41.599 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:41.599 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:41.599 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2015, 128])
2024-11-28 11:54:41.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3814
2024-11-28 11:54:41.600 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3815])
2024-11-28 11:54:41.867 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:41.913 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:41.913 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:41.913 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:41.913 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2375, 128])
2024-11-28 11:54:41.914 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3994
2024-11-28 11:54:41.914 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3995])
2024-11-28 11:54:42.160 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:42.201 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:42.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:42.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:42.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2555, 128])
2024-11-28 11:54:42.204 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4714
2024-11-28 11:54:42.204 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4715])
2024-11-28 11:54:42.537 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:42.589 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:42.590 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:42.590 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:42.590 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3275, 128])
2024-11-28 11:54:42.593 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5434
2024-11-28 11:54:42.593 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5435])
2024-11-28 11:54:42.947 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:43.000 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:43.000 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:43.000 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:43.000 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3995, 128])
2024-11-28 11:54:43.004 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6154
2024-11-28 11:54:43.004 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 6155])
2024-11-28 11:54:43.372 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:43.426 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:43.426 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:43.426 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:43.426 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4715, 128])
2024-11-28 11:54:43.429 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6514
2024-11-28 11:54:43.429 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6515])
2024-11-28 11:54:43.747 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:43.794 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:43.794 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:43.794 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:43.794 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5075, 128])
2024-11-28 11:54:43.797 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6694
2024-11-28 11:54:43.797 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6695])
2024-11-28 11:54:44.089 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:44.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:44.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:44.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:44.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5255, 128])
2024-11-28 11:54:44.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7414
2024-11-28 11:54:44.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7415])
2024-11-28 11:54:44.528 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:44.583 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:44.583 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:44.583 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:44.583 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5975, 128])
2024-11-28 11:54:44.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7774
2024-11-28 11:54:44.588 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 7775])
2024-11-28 11:54:44.930 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:44.978 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:44.978 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:44.978 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:44.978 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6335, 128])
2024-11-28 11:54:44.983 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7954
2024-11-28 11:54:44.983 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 7955])
2024-11-28 11:54:45.296 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:45.340 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:45.340 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:45.340 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:45.340 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6515, 128])
2024-11-28 11:54:45.348 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8674
2024-11-28 11:54:45.348 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8675])
2024-11-28 11:54:45.767 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:45.823 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:45.823 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:45.823 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:45.823 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7235, 128])
2024-11-28 11:54:45.828 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8854
2024-11-28 11:54:45.828 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8855])
2024-11-28 11:54:46.157 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:46.201 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:46.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:46.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:46.202 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7415, 128])
2024-11-28 11:54:46.211 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:54:46.211 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9575])
2024-11-28 11:54:46.648 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:46.705 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:46.705 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:46.705 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:46.705 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:54:46.711 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9754
2024-11-28 11:54:46.712 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9755])
2024-11-28 11:54:47.055 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:47.100 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:47.100 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:47.101 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:47.101 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8315, 128])
2024-11-28 11:54:47.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10114
2024-11-28 11:54:47.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 10115])
2024-11-28 11:54:47.494 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:47.545 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:47.545 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:47.545 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:47.545 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8675, 128])
2024-11-28 11:54:47.554 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10474
2024-11-28 11:54:47.554 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 10475])
2024-11-28 11:54:47.945 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:47.996 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:47.996 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:47.996 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:47.996 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9035, 128])
2024-11-28 11:54:48.008 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11194
2024-11-28 11:54:48.008 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 11195])
2024-11-28 11:54:48.479 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:48.537 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:48.537 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:54:48.537 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:54:48.537 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9755, 128])
2024-11-28 11:54:48.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11554
2024-11-28 11:54:48.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 11555])
2024-11-28 11:54:48.959 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:54:49.011 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:49.011 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:54:49.011 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:54:49.011 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10115, 128])
2024-11-28 11:54:49.020 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11734
2024-11-28 11:54:49.020 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 11735])
2024-11-28 11:54:49.397 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:54:49.444 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:49.444 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:54:49.444 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:54:49.444 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10295, 128])
2024-11-28 11:54:49.457 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12454
2024-11-28 11:54:49.457 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 12455])
2024-11-28 11:54:49.956 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:54:50.015 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:50.015 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 972])
2024-11-28 11:54:50.015 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 108
2024-11-28 11:54:50.015 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11015, 128])
2024-11-28 11:54:50.020 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11986
2024-11-28 11:54:50.020 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 972, 11987])
2024-11-28 11:54:50.286 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 972, 128])
2024-11-28 11:54:50.319 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:50.319 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 45])
2024-11-28 11:54:50.319 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:50.319 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11123, 128])
2024-11-28 11:54:50.319 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11167
2024-11-28 11:54:50.319 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 45, 11168])
2024-11-28 11:54:50.370 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 45, 128])
2024-11-28 11:54:50.378 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:50.378 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:50.378 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:50.378 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11168, 128])
2024-11-28 11:54:50.378 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11168
2024-11-28 11:54:50.378 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11169])
2024-11-28 11:54:50.425 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:54:50.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:50.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:50.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:50.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11169, 128])
2024-11-28 11:54:50.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11169
2024-11-28 11:54:50.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11170])
2024-11-28 11:54:50.478 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:54:50.485 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:50.485 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:50.485 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:50.485 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11170, 128])
2024-11-28 11:54:50.485 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11170
2024-11-28 11:54:50.485 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11171])
2024-11-28 11:54:50.531 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:54:50.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:54:50.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:54:50.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:54:50.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11171, 128])
2024-11-28 11:54:50.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11171
2024-11-28 11:54:50.547 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11172])
2024-11-28 11:54:50.593 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (B) Undersea
Pred B) Undersea
##########
2222222 B B
11111111111111 B B
Part  Acc: 88.89%
------------------------------ topic_reasoning ------------------------------
  3%|                                                                        | 9/264 [03:25<1:32:09, 21.68s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the main setting of the video?
Options:
(A) Desert
(B) Ocean
(C) City
(D) Palace
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:55:06.225 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:06.225 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:55:06.225 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:06.225 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:55:06.225 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:55:06.250 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:55:06.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:06.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:06.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:06.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:55:06.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1834
2024-11-28 11:55:06.257 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 1835])
2024-11-28 11:55:06.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:06.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:06.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:06.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:06.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 395, 128])
2024-11-28 11:55:06.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2014
2024-11-28 11:55:06.533 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 2015])
2024-11-28 11:55:06.749 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:06.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:06.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:06.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:06.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 575, 128])
2024-11-28 11:55:06.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2374
2024-11-28 11:55:06.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2375])
2024-11-28 11:55:07.031 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:07.075 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:07.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:07.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:07.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 935, 128])
2024-11-28 11:55:07.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2734
2024-11-28 11:55:07.076 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2735])
2024-11-28 11:55:07.325 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:07.370 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:07.370 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:07.370 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:07.370 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1295, 128])
2024-11-28 11:55:07.371 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3454
2024-11-28 11:55:07.371 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3455])
2024-11-28 11:55:07.678 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:07.729 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:07.729 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:07.729 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:07.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2015, 128])
2024-11-28 11:55:07.730 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3814
2024-11-28 11:55:07.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3815])
2024-11-28 11:55:07.998 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:08.044 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:08.044 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:08.044 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:08.044 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2375, 128])
2024-11-28 11:55:08.047 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:55:08.047 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4535])
2024-11-28 11:55:08.378 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:08.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:08.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:08.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:08.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:55:08.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4894
2024-11-28 11:55:08.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4895])
2024-11-28 11:55:08.723 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:08.770 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:08.770 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:08.770 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:08.770 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3455, 128])
2024-11-28 11:55:08.773 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5614
2024-11-28 11:55:08.773 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5615])
2024-11-28 11:55:09.126 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:09.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:09.180 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:09.180 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:09.180 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4175, 128])
2024-11-28 11:55:09.183 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5974
2024-11-28 11:55:09.183 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5975])
2024-11-28 11:55:09.491 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:09.538 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:09.538 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:09.538 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:09.538 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4535, 128])
2024-11-28 11:55:09.542 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6334
2024-11-28 11:55:09.542 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6335])
2024-11-28 11:55:09.857 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:09.904 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:09.904 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:09.904 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:09.904 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4895, 128])
2024-11-28 11:55:09.910 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7054
2024-11-28 11:55:09.910 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7055])
2024-11-28 11:55:10.295 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:10.349 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:10.350 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:10.350 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:10.350 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5615, 128])
2024-11-28 11:55:10.353 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7234
2024-11-28 11:55:10.353 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 7235])
2024-11-28 11:55:10.655 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:10.698 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:10.698 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:10.699 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:10.699 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5795, 128])
2024-11-28 11:55:10.703 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7594
2024-11-28 11:55:10.703 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 7595])
2024-11-28 11:55:11.041 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:11.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:11.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:11.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:11.090 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6155, 128])
2024-11-28 11:55:11.094 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7774
2024-11-28 11:55:11.094 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 7775])
2024-11-28 11:55:11.405 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:11.449 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:11.449 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:11.449 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:11.449 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6335, 128])
2024-11-28 11:55:11.457 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8494
2024-11-28 11:55:11.457 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8495])
2024-11-28 11:55:11.870 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:11.926 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:11.926 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:11.926 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:11.926 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7055, 128])
2024-11-28 11:55:11.935 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9214
2024-11-28 11:55:11.935 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9215])
2024-11-28 11:55:12.364 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:12.421 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:12.421 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:12.421 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:12.421 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7775, 128])
2024-11-28 11:55:12.428 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:55:12.428 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9575])
2024-11-28 11:55:12.803 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:12.853 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:12.853 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:12.853 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:12.853 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:55:12.864 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10294
2024-11-28 11:55:12.864 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10295])
2024-11-28 11:55:13.316 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:13.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:13.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:13.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:13.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8855, 128])
2024-11-28 11:55:13.385 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11014
2024-11-28 11:55:13.385 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 11015])
2024-11-28 11:55:13.856 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:13.914 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:13.914 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:13.914 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:13.914 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9575, 128])
2024-11-28 11:55:13.924 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11374
2024-11-28 11:55:13.924 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 11375])
2024-11-28 11:55:14.332 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:14.384 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:14.385 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:14.385 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:14.385 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9935, 128])
2024-11-28 11:55:14.397 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12094
2024-11-28 11:55:14.397 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 12095])
2024-11-28 11:55:14.889 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:14.948 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:14.948 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:14.948 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:14.948 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10655, 128])
2024-11-28 11:55:14.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12274
2024-11-28 11:55:14.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 12275])
2024-11-28 11:55:15.343 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:15.391 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:15.391 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:15.391 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:15.391 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10835, 128])
2024-11-28 11:55:15.405 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12994
2024-11-28 11:55:15.405 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 12995])
2024-11-28 11:55:15.917 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:15.977 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:15.977 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:15.977 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:15.977 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11555, 128])
2024-11-28 11:55:15.992 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 13714
2024-11-28 11:55:15.992 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 13715])
2024-11-28 11:55:16.522 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:16.583 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:16.583 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1080])
2024-11-28 11:55:16.583 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 216
2024-11-28 11:55:16.583 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 12275, 128])
2024-11-28 11:55:16.589 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 13354
2024-11-28 11:55:16.589 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1080, 13355])
2024-11-28 11:55:16.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1080, 128])
2024-11-28 11:55:16.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:16.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 45])
2024-11-28 11:55:16.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:16.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 12491, 128])
2024-11-28 11:55:16.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12535
2024-11-28 11:55:16.920 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 45, 12536])
2024-11-28 11:55:16.976 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 45, 128])
2024-11-28 11:55:16.984 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:16.984 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:16.984 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:16.984 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 12536, 128])
2024-11-28 11:55:16.984 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12536
2024-11-28 11:55:16.985 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 12537])
2024-11-28 11:55:17.034 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:17.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:17.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:17.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:17.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 12537, 128])
2024-11-28 11:55:17.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12537
2024-11-28 11:55:17.051 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 12538])
2024-11-28 11:55:17.101 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:17.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:17.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:17.108 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:17.109 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 12538, 128])
2024-11-28 11:55:17.109 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12538
2024-11-28 11:55:17.109 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 12539])
2024-11-28 11:55:17.158 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (D) Palace
Pred D) Palace
##########
2222222 D D
11111111111111 D D
Part  Acc: 90.00%
------------------------------ topic_reasoning ------------------------------
  4%|                                                                       | 10/264 [03:51<1:38:10, 23.19s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the setting of the scene in the video?
Options:
(A) City
(B) Island
(C) Snowy Mountain
(D) Forest
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:55:30.271 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:30.271 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:55:30.271 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:30.271 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:55:30.271 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:55:30.296 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:55:30.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:30.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:30.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:30.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:55:30.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 1654
2024-11-28 11:55:30.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 1655])
2024-11-28 11:55:30.510 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:30.550 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:30.550 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:30.550 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:30.550 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 215, 128])
2024-11-28 11:55:30.551 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2374
2024-11-28 11:55:30.551 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 2375])
2024-11-28 11:55:30.831 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:30.882 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:30.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:30.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:30.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 935, 128])
2024-11-28 11:55:30.884 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3094
2024-11-28 11:55:30.884 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3095])
2024-11-28 11:55:31.180 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:31.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:31.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:31.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:31.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1655, 128])
2024-11-28 11:55:31.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3274
2024-11-28 11:55:31.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3275])
2024-11-28 11:55:31.464 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:31.505 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:31.505 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:31.505 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:31.505 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1835, 128])
2024-11-28 11:55:31.506 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3454
2024-11-28 11:55:31.506 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3455])
2024-11-28 11:55:31.740 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:31.781 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:31.781 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:31.781 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:31.781 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2015, 128])
2024-11-28 11:55:31.782 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3814
2024-11-28 11:55:31.782 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3815])
2024-11-28 11:55:32.050 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:32.096 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:32.096 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:32.096 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:32.096 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2375, 128])
2024-11-28 11:55:32.098 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4174
2024-11-28 11:55:32.098 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4175])
2024-11-28 11:55:32.372 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:32.417 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:32.417 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:32.417 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:32.418 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2735, 128])
2024-11-28 11:55:32.419 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:55:32.419 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4535])
2024-11-28 11:55:32.700 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:32.746 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:32.746 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:32.746 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:32.746 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:55:32.748 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4894
2024-11-28 11:55:32.748 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 4895])
2024-11-28 11:55:33.037 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:33.083 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:33.083 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:33.083 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:33.083 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3455, 128])
2024-11-28 11:55:33.086 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5254
2024-11-28 11:55:33.086 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5255])
2024-11-28 11:55:33.381 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:33.428 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:33.429 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:33.429 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:33.429 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3815, 128])
2024-11-28 11:55:33.432 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5974
2024-11-28 11:55:33.433 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5975])
2024-11-28 11:55:33.794 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:33.847 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:33.847 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:33.847 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:33.847 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4535, 128])
2024-11-28 11:55:33.852 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6694
2024-11-28 11:55:33.852 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 6695])
2024-11-28 11:55:34.230 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:34.285 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:34.285 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:34.285 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:34.285 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5255, 128])
2024-11-28 11:55:34.291 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7414
2024-11-28 11:55:34.291 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7415])
2024-11-28 11:55:34.682 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:34.737 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:34.737 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:34.737 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:34.737 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5975, 128])
2024-11-28 11:55:34.741 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7594
2024-11-28 11:55:34.741 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 7595])
2024-11-28 11:55:35.050 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:35.093 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:35.093 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:35.094 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:35.094 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6155, 128])
2024-11-28 11:55:35.098 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7954
2024-11-28 11:55:35.099 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 7955])
2024-11-28 11:55:35.444 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:35.492 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:35.493 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:35.493 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:35.493 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6515, 128])
2024-11-28 11:55:35.498 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8314
2024-11-28 11:55:35.498 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 8315])
2024-11-28 11:55:35.849 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:35.898 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:35.898 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:35.898 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:35.898 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6875, 128])
2024-11-28 11:55:35.904 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8674
2024-11-28 11:55:35.905 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 8675])
2024-11-28 11:55:36.263 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:36.312 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:36.312 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:36.312 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:36.312 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7235, 128])
2024-11-28 11:55:36.317 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8854
2024-11-28 11:55:36.317 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8855])
2024-11-28 11:55:36.647 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:36.691 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:36.691 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:36.692 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:36.692 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7415, 128])
2024-11-28 11:55:36.701 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:55:36.701 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9575])
2024-11-28 11:55:37.139 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:37.196 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:37.196 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:37.196 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:37.196 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:55:37.206 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10294
2024-11-28 11:55:37.206 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10295])
2024-11-28 11:55:37.658 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:37.716 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:37.716 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:37.716 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:37.716 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8855, 128])
2024-11-28 11:55:37.727 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11014
2024-11-28 11:55:37.727 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 11015])
2024-11-28 11:55:38.196 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:38.255 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:38.255 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:38.255 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:38.255 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9575, 128])
2024-11-28 11:55:38.264 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11374
2024-11-28 11:55:38.265 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 11375])
2024-11-28 11:55:38.673 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:38.725 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:38.725 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:38.725 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:38.725 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9935, 128])
2024-11-28 11:55:38.734 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11554
2024-11-28 11:55:38.734 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 11555])
2024-11-28 11:55:39.109 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:39.155 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:39.156 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:39.156 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:39.156 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10115, 128])
2024-11-28 11:55:39.166 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11914
2024-11-28 11:55:39.166 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 11915])
2024-11-28 11:55:39.586 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:39.638 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:39.639 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:39.639 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:39.639 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10475, 128])
2024-11-28 11:55:39.649 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12274
2024-11-28 11:55:39.649 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 12275])
2024-11-28 11:55:40.074 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:40.127 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:40.127 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 972])
2024-11-28 11:55:40.127 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 108
2024-11-28 11:55:40.127 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10835, 128])
2024-11-28 11:55:40.131 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11806
2024-11-28 11:55:40.131 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 972, 11807])
2024-11-28 11:55:40.397 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 972, 128])
2024-11-28 11:55:40.429 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:40.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 49])
2024-11-28 11:55:40.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:40.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10943, 128])
2024-11-28 11:55:40.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10991
2024-11-28 11:55:40.430 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 49, 10992])
2024-11-28 11:55:40.481 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 49, 128])
2024-11-28 11:55:40.490 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:40.490 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:40.490 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:40.491 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10992, 128])
2024-11-28 11:55:40.491 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10992
2024-11-28 11:55:40.491 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10993])
2024-11-28 11:55:40.536 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:40.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:40.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:40.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:40.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10993, 128])
2024-11-28 11:55:40.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10993
2024-11-28 11:55:40.544 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10994])
2024-11-28 11:55:40.590 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:40.597 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:40.597 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:40.598 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:40.598 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10994, 128])
2024-11-28 11:55:40.598 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10994
2024-11-28 11:55:40.598 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 10995])
2024-11-28 11:55:40.643 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (A) City
Pred B) Island
##########
2222222 B A
Part  Acc: 81.82%
------------------------------ topic_reasoning ------------------------------
  4%|                                                                       | 11/264 [04:15<1:38:09, 23.28s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: Who is the main character shown in the video?
Options:
(A) A man in a red coat
(B) A woman in a green coat
(C) A woman in a blue coat
(D) A woman in a red coat
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:55:46.228 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:46.228 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:55:46.228 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:46.228 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:55:46.228 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:55:46.253 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:55:46.260 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:46.260 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:46.260 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:46.261 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:55:46.261 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2194
2024-11-28 11:55:46.261 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 2195])
2024-11-28 11:55:46.545 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:46.596 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:46.596 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:46.596 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:46.596 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 755, 128])
2024-11-28 11:55:46.596 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2554
2024-11-28 11:55:46.596 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2555])
2024-11-28 11:55:46.838 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:46.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:46.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:46.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:46.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1115, 128])
2024-11-28 11:55:46.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2914
2024-11-28 11:55:46.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 2915])
2024-11-28 11:55:47.133 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:47.178 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:47.178 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:47.178 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:47.178 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1475, 128])
2024-11-28 11:55:47.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3274
2024-11-28 11:55:47.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3275])
2024-11-28 11:55:47.436 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:47.481 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:47.481 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:47.482 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:47.482 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1835, 128])
2024-11-28 11:55:47.482 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3454
2024-11-28 11:55:47.482 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 3455])
2024-11-28 11:55:47.718 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:47.758 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:47.758 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:47.758 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:47.758 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2015, 128])
2024-11-28 11:55:47.761 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4174
2024-11-28 11:55:47.761 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 4175])
2024-11-28 11:55:48.082 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:48.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:48.135 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:48.135 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:48.135 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2735, 128])
2024-11-28 11:55:48.136 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4354
2024-11-28 11:55:48.136 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4355])
2024-11-28 11:55:48.390 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:48.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:48.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:48.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:48.431 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2915, 128])
2024-11-28 11:55:48.434 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5074
2024-11-28 11:55:48.434 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5075])
2024-11-28 11:55:48.776 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:48.829 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:48.829 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:48.829 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:48.829 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3635, 128])
2024-11-28 11:55:48.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5434
2024-11-28 11:55:48.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5435])
2024-11-28 11:55:49.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:49.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:49.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:49.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:49.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3995, 128])
2024-11-28 11:55:49.182 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5794
2024-11-28 11:55:49.182 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 5795])
2024-11-28 11:55:49.488 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:49.535 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:49.535 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:49.535 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:49.535 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4355, 128])
2024-11-28 11:55:49.538 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5974
2024-11-28 11:55:49.538 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 5975])
2024-11-28 11:55:49.819 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:49.861 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:49.861 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:49.861 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:49.861 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4535, 128])
2024-11-28 11:55:49.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6334
2024-11-28 11:55:49.865 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6335])
2024-11-28 11:55:50.179 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:50.227 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:50.227 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:50.227 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:50.227 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4895, 128])
2024-11-28 11:55:50.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7054
2024-11-28 11:55:50.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7055])
2024-11-28 11:55:50.617 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:50.671 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:50.671 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:50.671 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:50.671 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5615, 128])
2024-11-28 11:55:50.678 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7774
2024-11-28 11:55:50.678 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7775])
2024-11-28 11:55:51.077 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:51.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:51.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:51.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:51.132 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6335, 128])
2024-11-28 11:55:51.137 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8134
2024-11-28 11:55:51.137 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 8135])
2024-11-28 11:55:51.485 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:51.535 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:51.535 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:51.535 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:51.535 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6695, 128])
2024-11-28 11:55:51.539 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8314
2024-11-28 11:55:51.539 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 8315])
2024-11-28 11:55:51.859 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:51.903 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:51.903 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:51.903 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:51.903 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6875, 128])
2024-11-28 11:55:51.912 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9034
2024-11-28 11:55:51.912 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9035])
2024-11-28 11:55:52.338 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:52.394 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:52.394 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:55:52.394 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:55:52.394 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7595, 128])
2024-11-28 11:55:52.400 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9214
2024-11-28 11:55:52.400 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 9215])
2024-11-28 11:55:52.736 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:55:52.780 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:52.781 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:52.781 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:52.781 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7775, 128])
2024-11-28 11:55:52.788 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9574
2024-11-28 11:55:52.788 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 9575])
2024-11-28 11:55:53.162 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:53.212 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:53.212 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:53.212 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:53.212 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8135, 128])
2024-11-28 11:55:53.223 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10294
2024-11-28 11:55:53.223 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 10295])
2024-11-28 11:55:53.674 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:53.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:53.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:53.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:53.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8855, 128])
2024-11-28 11:55:53.743 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11014
2024-11-28 11:55:53.743 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 11015])
2024-11-28 11:55:54.211 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:54.269 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:54.269 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:54.269 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:54.269 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9575, 128])
2024-11-28 11:55:54.278 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11374
2024-11-28 11:55:54.278 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 11375])
2024-11-28 11:55:54.687 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:54.739 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:54.739 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:54.739 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:54.739 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9935, 128])
2024-11-28 11:55:54.752 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12094
2024-11-28 11:55:54.752 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 12095])
2024-11-28 11:55:55.243 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:55.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:55.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:55:55.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:55:55.303 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10655, 128])
2024-11-28 11:55:55.317 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12814
2024-11-28 11:55:55.317 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 12815])
2024-11-28 11:55:55.825 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:55:55.885 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:55.885 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:55:55.885 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:55:55.885 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11375, 128])
2024-11-28 11:55:55.896 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 13174
2024-11-28 11:55:55.897 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 13175])
2024-11-28 11:55:56.339 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:55:56.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:56.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 972])
2024-11-28 11:55:56.392 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 108
2024-11-28 11:55:56.393 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11735, 128])
2024-11-28 11:55:56.397 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12706
2024-11-28 11:55:56.397 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 972, 12707])
2024-11-28 11:55:56.675 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 972, 128])
2024-11-28 11:55:56.708 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:56.708 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 66])
2024-11-28 11:55:56.708 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:56.708 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11843, 128])
2024-11-28 11:55:56.709 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11908
2024-11-28 11:55:56.709 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 66, 11909])
2024-11-28 11:55:56.767 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 66, 128])
2024-11-28 11:55:56.783 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:56.784 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:56.784 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:56.784 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11909, 128])
2024-11-28 11:55:56.784 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11909
2024-11-28 11:55:56.784 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11910])
2024-11-28 11:55:56.832 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:56.839 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:56.839 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:56.839 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:56.839 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11910, 128])
2024-11-28 11:55:56.839 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11910
2024-11-28 11:55:56.839 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11911])
2024-11-28 11:55:56.887 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:56.899 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:56.900 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:56.900 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:56.900 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11911, 128])
2024-11-28 11:55:56.900 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11911
2024-11-28 11:55:56.900 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11912])
2024-11-28 11:55:56.949 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:56.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:56.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:56.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:56.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11912, 128])
2024-11-28 11:55:56.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11912
2024-11-28 11:55:56.957 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11913])
2024-11-28 11:55:57.005 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:57.025 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:57.025 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:57.025 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:57.025 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11913, 128])
2024-11-28 11:55:57.026 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11913
2024-11-28 11:55:57.026 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11914])
2024-11-28 11:55:57.074 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:57.081 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:57.081 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:57.081 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:57.081 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11914, 128])
2024-11-28 11:55:57.082 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11914
2024-11-28 11:55:57.082 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11915])
2024-11-28 11:55:57.130 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:57.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:57.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:57.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:57.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11915, 128])
2024-11-28 11:55:57.138 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11915
2024-11-28 11:55:57.139 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11916])
2024-11-28 11:55:57.187 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:55:57.200 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:55:57.200 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:55:57.200 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:55:57.200 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11916, 128])
2024-11-28 11:55:57.200 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11916
2024-11-28 11:55:57.200 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11917])
2024-11-28 11:55:57.249 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (D) A woman in a red coat
Pred D) A woman in a red coat
##########
2222222 D D
11111111111111 D D
Part  Acc: 83.33%
------------------------------ topic_reasoning ------------------------------
  5%|                                                                      | 12/264 [04:31<1:29:14, 21.25s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What type of video is this?
Options:
(A) Science Fiction
(B) Animals
(C) Romance
(D) Comedy
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
2024-11-28 11:56:07.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:07.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 35])
2024-11-28 11:56:07.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:56:07.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 34
2024-11-28 11:56:07.489 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 35, 35])
2024-11-28 11:56:07.514 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 35, 128])
2024-11-28 11:56:07.520 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:07.521 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:07.521 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:07.521 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 35, 128])
2024-11-28 11:56:07.521 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2194
2024-11-28 11:56:07.521 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 2195])
2024-11-28 11:56:07.798 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:07.848 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:07.848 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:07.849 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:07.849 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 755, 128])
2024-11-28 11:56:07.850 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 2914
2024-11-28 11:56:07.850 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 2915])
2024-11-28 11:56:08.143 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:08.194 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:08.194 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:08.194 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:08.194 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 1475, 128])
2024-11-28 11:56:08.196 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3634
2024-11-28 11:56:08.196 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 3635])
2024-11-28 11:56:08.505 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:08.557 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:08.557 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:56:08.557 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:56:08.557 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2195, 128])
2024-11-28 11:56:08.558 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 3994
2024-11-28 11:56:08.559 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 3995])
2024-11-28 11:56:08.830 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:56:08.875 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:08.875 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:08.876 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:08.876 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2555, 128])
2024-11-28 11:56:08.876 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4174
2024-11-28 11:56:08.876 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4175])
2024-11-28 11:56:09.126 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:09.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:09.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:09.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:09.168 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2735, 128])
2024-11-28 11:56:09.169 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4354
2024-11-28 11:56:09.169 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4355])
2024-11-28 11:56:09.421 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:09.463 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:09.463 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:09.463 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:09.463 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 2915, 128])
2024-11-28 11:56:09.465 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4534
2024-11-28 11:56:09.465 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4535])
2024-11-28 11:56:09.721 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:09.763 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:09.763 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:09.763 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:09.763 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3095, 128])
2024-11-28 11:56:09.764 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 4714
2024-11-28 11:56:09.764 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 4715])
2024-11-28 11:56:10.023 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:10.065 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:10.065 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:10.065 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:10.065 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3275, 128])
2024-11-28 11:56:10.068 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 5434
2024-11-28 11:56:10.068 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 5435])
2024-11-28 11:56:10.417 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:10.470 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:10.471 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:10.471 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:10.471 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 3995, 128])
2024-11-28 11:56:10.475 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6154
2024-11-28 11:56:10.475 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 6155])
2024-11-28 11:56:10.840 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:10.893 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:10.894 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:56:10.894 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:56:10.894 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 4715, 128])
2024-11-28 11:56:10.897 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6514
2024-11-28 11:56:10.897 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 6515])
2024-11-28 11:56:11.215 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:56:11.263 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:11.263 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:11.263 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:11.263 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5075, 128])
2024-11-28 11:56:11.266 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6694
2024-11-28 11:56:11.266 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6695])
2024-11-28 11:56:11.559 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:11.602 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:11.602 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:11.602 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:11.602 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5255, 128])
2024-11-28 11:56:11.605 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 6874
2024-11-28 11:56:11.605 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 6875])
2024-11-28 11:56:11.901 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:11.944 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:11.944 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:11.944 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:11.945 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 5435, 128])
2024-11-28 11:56:11.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 7594
2024-11-28 11:56:11.951 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 7595])
2024-11-28 11:56:12.351 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:12.406 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:12.406 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:12.406 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:12.406 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6155, 128])
2024-11-28 11:56:12.413 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 8314
2024-11-28 11:56:12.413 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 8315])
2024-11-28 11:56:12.825 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:12.880 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:12.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:12.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:12.881 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 6875, 128])
2024-11-28 11:56:12.889 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9034
2024-11-28 11:56:12.889 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9035])
2024-11-28 11:56:13.318 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:13.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:13.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:13.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:13.374 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 7595, 128])
2024-11-28 11:56:13.383 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 9754
2024-11-28 11:56:13.383 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 9755])
2024-11-28 11:56:13.825 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:13.882 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:13.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:56:13.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:56:13.883 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8315, 128])
2024-11-28 11:56:13.891 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10114
2024-11-28 11:56:13.891 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 10115])
2024-11-28 11:56:14.277 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:56:14.327 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:14.328 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:56:14.328 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:56:14.328 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 8675, 128])
2024-11-28 11:56:14.336 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10474
2024-11-28 11:56:14.336 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 10475])
2024-11-28 11:56:14.729 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:56:14.780 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:14.780 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:56:14.780 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:56:14.780 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9035, 128])
2024-11-28 11:56:14.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 10834
2024-11-28 11:56:14.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 10835])
2024-11-28 11:56:15.187 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:56:15.238 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:15.238 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 2160])
2024-11-28 11:56:15.238 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 720
2024-11-28 11:56:15.238 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 9395, 128])
2024-11-28 11:56:15.250 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11554
2024-11-28 11:56:15.250 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 2160, 11555])
2024-11-28 11:56:15.731 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 2160, 128])
2024-11-28 11:56:15.789 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:15.790 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:15.790 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:15.790 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10115, 128])
2024-11-28 11:56:15.798 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11734
2024-11-28 11:56:15.798 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 11735])
2024-11-28 11:56:16.174 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:16.221 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:16.221 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1800])
2024-11-28 11:56:16.221 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 360
2024-11-28 11:56:16.221 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10295, 128])
2024-11-28 11:56:16.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12094
2024-11-28 11:56:16.232 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1800, 12095])
2024-11-28 11:56:16.653 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1800, 128])
2024-11-28 11:56:16.706 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:16.706 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:16.706 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:16.706 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10655, 128])
2024-11-28 11:56:16.715 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12274
2024-11-28 11:56:16.715 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 12275])
2024-11-28 11:56:17.100 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:17.148 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:17.148 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1620])
2024-11-28 11:56:17.148 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 180
2024-11-28 11:56:17.148 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 10835, 128])
2024-11-28 11:56:17.157 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12454
2024-11-28 11:56:17.157 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1620, 12455])
2024-11-28 11:56:17.546 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1620, 128])
2024-11-28 11:56:17.594 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:17.594 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1296])
2024-11-28 11:56:17.594 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 432
2024-11-28 11:56:17.594 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11015, 128])
2024-11-28 11:56:17.601 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 12310
2024-11-28 11:56:17.601 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1296, 12311])
2024-11-28 11:56:17.921 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1296, 128])
2024-11-28 11:56:17.962 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:17.962 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 44])
2024-11-28 11:56:17.962 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:56:17.962 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11447, 128])
2024-11-28 11:56:17.962 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11490
2024-11-28 11:56:17.963 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 44, 11491])
2024-11-28 11:56:18.015 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 44, 128])
2024-11-28 11:56:18.024 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:18.024 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:56:18.024 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:56:18.024 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11491, 128])
2024-11-28 11:56:18.024 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11491
2024-11-28 11:56:18.024 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11492])
2024-11-28 11:56:18.071 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:56:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:56:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:56:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11492, 128])
2024-11-28 11:56:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11492
2024-11-28 11:56:18.079 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11493])
2024-11-28 11:56:18.126 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
2024-11-28 11:56:18.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1975 - ####################################################################################################
2024-11-28 11:56:18.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1976 - step_input length: torch.Size([1, 1])
2024-11-28 11:56:18.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1977 - beacon number: 0
2024-11-28 11:56:18.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1979 - past key states shape: torch.Size([1, 4, 11493, 128])
2024-11-28 11:56:18.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1981 - layer idx: 0 position_ids: 0 - 11493
2024-11-28 11:56:18.134 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:1985 - layer idx: 0 attn_mask.shape: torch.Size([1, 1, 1, 11494])
2024-11-28 11:56:18.181 | DEBUG    | videoxl.model.language_model.llava_qwen:_beacon_forward:2007 - output.past_key_values[0]:  torch.Size([1, 4, 1, 128])
##########
GT (B) Animals
Pred B) Animals
##########
2222222 B B
11111111111111 B B
Part  Acc: 84.62%
------------------------------ topic_reasoning ------------------------------
  5%|                                                                      | 13/264 [04:52<1:28:29, 21.15s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the weather like in the video?
Options:
(A) Blizzard
(B) Overcast
(C) Sunny
(D) Hail
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
