2024-11-29 13:59:12.177 | INFO     | __main__:<module>:159 - This is an INFO message
########## /share/junjie/shuyan/VideoXL_weight_8
You are using a model of type qwen2 to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
_attn_implementation: sdpa
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.61s/it]
启用 reload 机制
使用的 top_k=3
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
  0%|          | 0/264 [00:00<?, ?it/s]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the main background of the video?
Options:
(A) Grassland
(B) Lake
(C) Ocean
(D) Desert
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
2024-11-29 14:00:01.306 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.307 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.326 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.327 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:00:01.327 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.327 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.328 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0043, 0.0043]], device='cuda:0')[0m
2024-11-29 14:00:01.328 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([8, 6, 0], device='cuda:0')[0m
2024-11-29 14:00:01.328 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.328 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15957[0m
2024-11-29 14:00:01.329 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.338 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.339 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.349 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.349 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:00:01.349 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.349 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.350 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0063, 0.0055, 0.0055]], device='cuda:0')[0m
2024-11-29 14:00:01.351 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.351 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.351 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.351 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.360 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.361 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:00:01.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.372 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0057, 0.0055, 0.0053]], device='cuda:0')[0m
2024-11-29 14:00:01.372 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 23, 15], device='cuda:0')[0m
2024-11-29 14:00:01.372 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.372 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.372 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.382 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.382 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.391 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.391 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:00:01.391 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.391 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.392 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0078, 0.0065, 0.0063]], device='cuda:0')[0m
2024-11-29 14:00:01.392 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.392 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.392 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.393 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.402 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.402 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.411 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.411 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:00:01.411 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.411 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.412 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0051, 0.0043, 0.0040]], device='cuda:0')[0m
2024-11-29 14:00:01.412 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.412 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.412 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.412 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.421 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.422 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.430 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.431 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:00:01.431 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.431 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.431 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0053, 0.0036, 0.0031]], device='cuda:0')[0m
2024-11-29 14:00:01.432 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.432 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.432 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.432 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.441 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.442 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:00:01.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0086, 0.0068, 0.0062]], device='cuda:0')[0m
2024-11-29 14:00:01.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.455 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.455 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.455 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.464 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.465 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.475 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.475 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:00:01.476 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.476 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.476 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0034, 0.0033]], device='cuda:0')[0m
2024-11-29 14:00:01.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.487 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.487 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.498 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.499 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:00:01.499 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.499 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.499 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0066, 0.0038, 0.0034]], device='cuda:0')[0m
2024-11-29 14:00:01.500 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.500 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.500 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.500 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.509 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.510 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.520 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.521 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:00:01.521 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.521 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.521 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0035, 0.0032]], device='cuda:0')[0m
2024-11-29 14:00:01.522 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  5, 11], device='cuda:0')[0m
2024-11-29 14:00:01.522 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.522 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.522 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.532 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.532 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.545 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.545 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:00:01.546 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.546 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0069, 0.0049, 0.0045]], device='cuda:0')[0m
2024-11-29 14:00:01.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.557 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.557 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.566 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.566 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:00:01.567 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.567 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.567 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0065, 0.0039, 0.0034]], device='cuda:0')[0m
2024-11-29 14:00:01.568 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.568 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.568 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.568 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.577 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.578 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.591 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.591 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:00:01.591 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.591 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0032, 0.0026]], device='cuda:0')[0m
2024-11-29 14:00:01.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  5, 11], device='cuda:0')[0m
2024-11-29 14:00:01.593 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.593 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.593 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.602 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.603 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.612 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.612 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:00:01.612 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.612 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.613 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0063, 0.0048, 0.0038]], device='cuda:0')[0m
2024-11-29 14:00:01.613 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  5, 11], device='cuda:0')[0m
2024-11-29 14:00:01.613 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.613 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.613 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.622 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.623 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:00:01.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.636 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.636 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0063, 0.0034, 0.0026]], device='cuda:0')[0m
2024-11-29 14:00:01.637 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 0], device='cuda:0')[0m
2024-11-29 14:00:01.637 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.637 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14877[0m
2024-11-29 14:00:01.637 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.647 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.647 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.656 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.656 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:00:01.656 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.656 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0069, 0.0049, 0.0041]], device='cuda:0')[0m
2024-11-29 14:00:01.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.667 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.667 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.675 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.676 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:00:01.676 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.676 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.676 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0062, 0.0036, 0.0025]], device='cuda:0')[0m
2024-11-29 14:00:01.677 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.677 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.677 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.677 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.686 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.687 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.697 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.697 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:00:01.697 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.697 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.698 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0029, 0.0024]], device='cuda:0')[0m
2024-11-29 14:00:01.698 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 0], device='cuda:0')[0m
2024-11-29 14:00:01.698 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.698 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14877[0m
2024-11-29 14:00:01.698 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.708 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.708 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.717 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.717 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:00:01.717 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.717 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.718 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0058, 0.0050, 0.0042]], device='cuda:0')[0m
2024-11-29 14:00:01.718 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  5, 12], device='cuda:0')[0m
2024-11-29 14:00:01.718 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.718 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.718 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.727 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.728 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.736 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.736 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:00:01.736 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.736 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.737 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0058, 0.0036, 0.0032]], device='cuda:0')[0m
2024-11-29 14:00:01.737 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 7], device='cuda:0')[0m
2024-11-29 14:00:01.737 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.738 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.738 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.747 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.747 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.756 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.756 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:00:01.756 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.756 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0038, 0.0033]], device='cuda:0')[0m
2024-11-29 14:00:01.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  5, 12], device='cuda:0')[0m
2024-11-29 14:00:01.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.758 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.767 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.767 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.779 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.779 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:00:01.779 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.779 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.780 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0073, 0.0042, 0.0034]], device='cuda:0')[0m
2024-11-29 14:00:01.780 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  5, 15], device='cuda:0')[0m
2024-11-29 14:00:01.780 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.781 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.781 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.790 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.790 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.802 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.802 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:00:01.802 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.802 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.803 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0024, 0.0020]], device='cuda:0')[0m
2024-11-29 14:00:01.804 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 0], device='cuda:0')[0m
2024-11-29 14:00:01.804 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.804 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14877[0m
2024-11-29 14:00:01.804 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.813 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.813 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.822 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.822 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:00:01.823 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.823 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.823 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0053, 0.0035, 0.0031]], device='cuda:0')[0m
2024-11-29 14:00:01.824 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  5, 15], device='cuda:0')[0m
2024-11-29 14:00:01.824 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.824 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.824 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.833 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.833 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:00:01.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.843 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0052, 0.0038, 0.0035]], device='cuda:0')[0m
2024-11-29 14:00:01.843 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  5, 15], device='cuda:0')[0m
2024-11-29 14:00:01.843 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.844 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14337[0m
2024-11-29 14:00:01.844 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.853 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.853 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.864 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.865 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:00:01.865 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.865 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.865 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0065, 0.0044, 0.0042]], device='cuda:0')[0m
2024-11-29 14:00:01.866 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 5, 0], device='cuda:0')[0m
2024-11-29 14:00:01.866 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.866 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14877[0m
2024-11-29 14:00:01.866 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.876 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.876 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.889 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.889 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:00:01.889 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.889 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.890 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0074, 0.0046, 0.0035]], device='cuda:0')[0m
2024-11-29 14:00:01.890 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 5], device='cuda:0')[0m
2024-11-29 14:00:01.891 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.891 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14877[0m
2024-11-29 14:00:01.891 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:01.900 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:01.901 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:01.913 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:01.913 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:00:01.913 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:01.913 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1295), (4, 1295, 1655), (5, 1655, 2015), (6, 2015, 2195), (7, 2195, 2915), (8, 2915, 3095), (9, 3095, 3815), (10, 3815, 4535), (11, 4535, 4895), (12, 4895, 5075), (13, 5075, 5255), (14, 5255, 5435), (15, 5435, 5615), (16, 5615, 5795), (17, 5795, 6155), (18, 6155, 6875), (19, 6875, 7055), (20, 7055, 7415), (21, 7415, 7775), (22, 7775, 8135), (23, 8135, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9431)][0m
2024-11-29 14:00:01.914 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0073, 0.0053, 0.0043]], device='cuda:0')[0m
2024-11-29 14:00:01.914 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 3], device='cuda:0')[0m
2024-11-29 14:00:01.914 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9477[0m
2024-11-29 14:00:01.914 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15057[0m
2024-11-29 14:00:01.915 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
##########
GT (A) Grassland
Pred A) Grassland
##########
2222222 A A
11111111111111 A A
Part  Acc: 100.00%
------------------------------ topic_reasoning ------------------------------
  0%|          | 1/264 [00:23<1:41:18, 23.11s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What color is the scarf worn by the woman in the video?
Options:
(A) Red
(B) Blue
(C) White
(D) Pink
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:00:26.289 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.289 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.305 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.306 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:00:26.306 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.306 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.307 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0045, 0.0045]], device='cuda:0')[0m
2024-11-29 14:00:26.307 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25,  6,  5], device='cuda:0')[0m
2024-11-29 14:00:26.307 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.307 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15312[0m
2024-11-29 14:00:26.307 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.317 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.318 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.332 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.332 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:00:26.333 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.333 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.333 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0073, 0.0069, 0.0057]], device='cuda:0')[0m
2024-11-29 14:00:26.334 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:00:26.334 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.334 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14916[0m
2024-11-29 14:00:26.334 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.345 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.345 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.357 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.357 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:00:26.358 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.358 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.358 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0060, 0.0057]], device='cuda:0')[0m
2024-11-29 14:00:26.359 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 16], device='cuda:0')[0m
2024-11-29 14:00:26.359 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.359 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.359 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.368 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.369 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:00:26.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0085, 0.0069, 0.0065]], device='cuda:0')[0m
2024-11-29 14:00:26.379 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 8, 0], device='cuda:0')[0m
2024-11-29 14:00:26.379 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.379 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.379 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.388 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.389 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.398 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.398 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:00:26.398 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.398 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0057, 0.0043, 0.0041]], device='cuda:0')[0m
2024-11-29 14:00:26.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  8, 22], device='cuda:0')[0m
2024-11-29 14:00:26.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14556[0m
2024-11-29 14:00:26.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.409 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.409 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.420 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.420 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:00:26.420 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.420 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.421 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0073, 0.0050, 0.0040]], device='cuda:0')[0m
2024-11-29 14:00:26.421 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  8, 19], device='cuda:0')[0m
2024-11-29 14:00:26.422 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.422 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14556[0m
2024-11-29 14:00:26.422 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.431 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.432 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.443 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.443 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:00:26.443 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.443 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.444 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0098, 0.0075, 0.0067]], device='cuda:0')[0m
2024-11-29 14:00:26.444 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 8, 0], device='cuda:0')[0m
2024-11-29 14:00:26.444 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.444 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.444 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.454 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.454 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.463 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.463 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:00:26.463 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.463 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.464 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0065, 0.0049, 0.0043]], device='cuda:0')[0m
2024-11-29 14:00:26.464 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 8, 0], device='cuda:0')[0m
2024-11-29 14:00:26.465 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.465 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.465 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.475 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.475 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.487 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.487 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:00:26.487 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.487 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.488 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0081, 0.0051, 0.0050]], device='cuda:0')[0m
2024-11-29 14:00:26.488 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 8], device='cuda:0')[0m
2024-11-29 14:00:26.488 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.488 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.488 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.498 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.498 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.507 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.507 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:00:26.507 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.508 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.508 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0072, 0.0053, 0.0051]], device='cuda:0')[0m
2024-11-29 14:00:26.509 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 8, 0], device='cuda:0')[0m
2024-11-29 14:00:26.509 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.509 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.509 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.518 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.518 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.529 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.529 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:00:26.529 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.529 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.530 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0077, 0.0057, 0.0052]], device='cuda:0')[0m
2024-11-29 14:00:26.530 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 8, 0], device='cuda:0')[0m
2024-11-29 14:00:26.531 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.531 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.531 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.540 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.540 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.554 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.554 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:00:26.554 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.554 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.555 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0074, 0.0056, 0.0049]], device='cuda:0')[0m
2024-11-29 14:00:26.555 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 8], device='cuda:0')[0m
2024-11-29 14:00:26.556 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.556 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.556 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.565 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.566 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.575 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.575 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:00:26.575 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.575 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.576 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0062, 0.0049, 0.0039]], device='cuda:0')[0m
2024-11-29 14:00:26.576 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 8], device='cuda:0')[0m
2024-11-29 14:00:26.576 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.577 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.577 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.586 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.586 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.595 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.596 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:00:26.596 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.596 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.596 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0070, 0.0055, 0.0052]], device='cuda:0')[0m
2024-11-29 14:00:26.597 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 19,  8], device='cuda:0')[0m
2024-11-29 14:00:26.597 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.597 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14556[0m
2024-11-29 14:00:26.597 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.607 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.607 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.618 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.618 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:00:26.618 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.618 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.619 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0080, 0.0068, 0.0034]], device='cuda:0')[0m
2024-11-29 14:00:26.619 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:00:26.619 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.620 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14916[0m
2024-11-29 14:00:26.620 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.629 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.629 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.638 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.639 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:00:26.639 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.639 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.639 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0072, 0.0065, 0.0053]], device='cuda:0')[0m
2024-11-29 14:00:26.640 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 19], device='cuda:0')[0m
2024-11-29 14:00:26.640 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.640 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.640 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.649 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.650 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.659 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.659 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:00:26.659 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.659 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.660 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0047, 0.0033]], device='cuda:0')[0m
2024-11-29 14:00:26.660 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 8], device='cuda:0')[0m
2024-11-29 14:00:26.660 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.660 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.660 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.669 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.670 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.679 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.679 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:00:26.679 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.679 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0042, 0.0038, 0.0030]], device='cuda:0')[0m
2024-11-29 14:00:26.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 19], device='cuda:0')[0m
2024-11-29 14:00:26.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.690 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.690 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.699 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.699 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:00:26.699 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.699 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.700 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0051, 0.0048, 0.0048]], device='cuda:0')[0m
2024-11-29 14:00:26.700 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 19], device='cuda:0')[0m
2024-11-29 14:00:26.700 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.700 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.700 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.709 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.710 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.724 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.724 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:00:26.724 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.724 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0059, 0.0036]], device='cuda:0')[0m
2024-11-29 14:00:26.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 8], device='cuda:0')[0m
2024-11-29 14:00:26.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.735 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.735 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.744 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.744 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:00:26.744 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.744 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.745 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0061, 0.0041]], device='cuda:0')[0m
2024-11-29 14:00:26.745 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 8], device='cuda:0')[0m
2024-11-29 14:00:26.745 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.745 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.745 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.755 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.755 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:00:26.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.765 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0067, 0.0065, 0.0039]], device='cuda:0')[0m
2024-11-29 14:00:26.765 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 8], device='cuda:0')[0m
2024-11-29 14:00:26.765 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.765 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.765 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.775 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.775 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.784 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.784 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:00:26.784 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.784 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.785 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0047, 0.0022]], device='cuda:0')[0m
2024-11-29 14:00:26.785 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 15], device='cuda:0')[0m
2024-11-29 14:00:26.785 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.785 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14736[0m
2024-11-29 14:00:26.785 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.795 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.795 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:00:26.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.808 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0059, 0.0034]], device='cuda:0')[0m
2024-11-29 14:00:26.808 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:00:26.808 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.808 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14916[0m
2024-11-29 14:00:26.809 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.818 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.818 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.828 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.828 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:00:26.828 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.828 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.829 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0058, 0.0056, 0.0038]], device='cuda:0')[0m
2024-11-29 14:00:26.829 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:00:26.829 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.829 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14916[0m
2024-11-29 14:00:26.829 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.839 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.839 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.849 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.850 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:00:26.850 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.850 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.850 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0080, 0.0076, 0.0048]], device='cuda:0')[0m
2024-11-29 14:00:26.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:00:26.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14916[0m
2024-11-29 14:00:26.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.860 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.861 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.872 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.872 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:00:26.872 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.872 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.873 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0096, 0.0087, 0.0040]], device='cuda:0')[0m
2024-11-29 14:00:26.873 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:00:26.873 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.873 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14916[0m
2024-11-29 14:00:26.873 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:26.883 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:26.883 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:26.896 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:26.896 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:00:26.896 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:26.896 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 1655), (6, 1655, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 3635), (10, 3635, 4355), (11, 4355, 4535), (12, 4535, 4715), (13, 4715, 5435), (14, 5435, 5795), (15, 5795, 6515), (16, 6515, 6695), (17, 6695, 6875), (18, 6875, 7235), (19, 7235, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 8855), (24, 8855, 9035), (25, 9035, 9215), (26, 9215, 9647)][0m
2024-11-29 14:00:26.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0090, 0.0077, 0.0056]], device='cuda:0')[0m
2024-11-29 14:00:26.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 6], device='cuda:0')[0m
2024-11-29 14:00:26.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9696[0m
2024-11-29 14:00:26.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15276[0m
2024-11-29 14:00:26.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (B) Blue
Pred C) White
##########
2222222 C B
Part  Acc: 50.00%
------------------------------ topic_reasoning ------------------------------
  1%|          | 2/264 [00:48<1:45:35, 24.18s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What type of film is this?
Options:
(A) Mystery
(B) Comedy
(C) Romance
(D) Action
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:00:52.110 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.110 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.120 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.120 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:00:52.120 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.120 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.121 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0037, 0.0037, 0.0037]], device='cuda:0')[0m
2024-11-29 14:00:52.121 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25,  5,  3], device='cuda:0')[0m
2024-11-29 14:00:52.121 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.121 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16206[0m
2024-11-29 14:00:52.121 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.131 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.131 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.140 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.140 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:00:52.140 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.140 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.141 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0047, 0.0047]], device='cuda:0')[0m
2024-11-29 14:00:52.141 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:00:52.141 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.141 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15990[0m
2024-11-29 14:00:52.141 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.150 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.150 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:00:52.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0052, 0.0051, 0.0049]], device='cuda:0')[0m
2024-11-29 14:00:52.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([11, 18,  7], device='cuda:0')[0m
2024-11-29 14:00:52.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15450[0m
2024-11-29 14:00:52.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.171 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.171 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.179 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.180 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:00:52.180 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.180 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.180 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0061, 0.0060, 0.0058]], device='cuda:0')[0m
2024-11-29 14:00:52.181 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 8, 11,  7], device='cuda:0')[0m
2024-11-29 14:00:52.181 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.181 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15450[0m
2024-11-29 14:00:52.181 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.190 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.190 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:00:52.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.200 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0038, 0.0038, 0.0035]], device='cuda:0')[0m
2024-11-29 14:00:52.200 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 8, 11,  9], device='cuda:0')[0m
2024-11-29 14:00:52.200 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.200 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15450[0m
2024-11-29 14:00:52.200 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.209 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.209 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.218 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.219 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:00:52.219 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.219 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.219 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0036, 0.0033, 0.0033]], device='cuda:0')[0m
2024-11-29 14:00:52.219 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  8, 11], device='cuda:0')[0m
2024-11-29 14:00:52.220 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.220 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.220 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.229 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.229 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.238 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.239 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:00:52.239 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.239 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.239 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0060, 0.0060]], device='cuda:0')[0m
2024-11-29 14:00:52.240 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  8, 11], device='cuda:0')[0m
2024-11-29 14:00:52.240 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.240 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.240 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.249 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.249 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.259 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.259 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:00:52.259 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.259 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.260 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0041, 0.0036, 0.0034]], device='cuda:0')[0m
2024-11-29 14:00:52.260 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0, 14, 11], device='cuda:0')[0m
2024-11-29 14:00:52.260 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.260 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.260 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.269 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.269 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.278 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.278 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:00:52.278 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.278 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.279 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0033, 0.0033]], device='cuda:0')[0m
2024-11-29 14:00:52.279 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  7, 11], device='cuda:0')[0m
2024-11-29 14:00:52.279 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.279 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.279 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.288 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.289 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.297 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:00:52.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0038, 0.0037]], device='cuda:0')[0m
2024-11-29 14:00:52.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  7, 11], device='cuda:0')[0m
2024-11-29 14:00:52.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.308 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.308 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.317 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.317 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:00:52.317 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.317 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.318 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0049, 0.0049, 0.0048]], device='cuda:0')[0m
2024-11-29 14:00:52.318 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  7, 11], device='cuda:0')[0m
2024-11-29 14:00:52.318 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.318 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.318 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.327 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.328 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.337 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.337 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:00:52.338 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.338 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.338 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0038, 0.0037]], device='cuda:0')[0m
2024-11-29 14:00:52.338 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 6, 7], device='cuda:0')[0m
2024-11-29 14:00:52.339 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.339 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.339 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.348 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.348 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.357 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.357 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:00:52.357 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.357 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.357 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0036, 0.0032, 0.0030]], device='cuda:0')[0m
2024-11-29 14:00:52.358 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 7, 6], device='cuda:0')[0m
2024-11-29 14:00:52.358 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.358 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.358 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.367 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.367 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.376 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.377 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:00:52.377 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.377 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.377 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0043, 0.0042]], device='cuda:0')[0m
2024-11-29 14:00:52.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([7, 0, 6], device='cuda:0')[0m
2024-11-29 14:00:52.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.387 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.387 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.396 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.396 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:00:52.396 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.396 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.397 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0030, 0.0026, 0.0026]], device='cuda:0')[0m
2024-11-29 14:00:52.397 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  7, 21], device='cuda:0')[0m
2024-11-29 14:00:52.397 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.398 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16170[0m
2024-11-29 14:00:52.398 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.407 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.407 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.416 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.417 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:00:52.417 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.417 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.417 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0041, 0.0041, 0.0039]], device='cuda:0')[0m
2024-11-29 14:00:52.417 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 7, 6], device='cuda:0')[0m
2024-11-29 14:00:52.418 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.418 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15630[0m
2024-11-29 14:00:52.418 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.427 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.427 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.438 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.438 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:00:52.439 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.439 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.439 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0031, 0.0030, 0.0021]], device='cuda:0')[0m
2024-11-29 14:00:52.439 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 2, 21,  0], device='cuda:0')[0m
2024-11-29 14:00:52.440 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.440 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16350[0m
2024-11-29 14:00:52.440 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.449 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.450 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.461 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.462 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:00:52.462 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.462 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.462 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0032, 0.0029, 0.0024]], device='cuda:0')[0m
2024-11-29 14:00:52.463 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([21,  2,  0], device='cuda:0')[0m
2024-11-29 14:00:52.463 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.463 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16350[0m
2024-11-29 14:00:52.463 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.472 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.472 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.481 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.482 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:00:52.482 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.482 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.482 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0041, 0.0037, 0.0035]], device='cuda:0')[0m
2024-11-29 14:00:52.483 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([2, 0, 7], device='cuda:0')[0m
2024-11-29 14:00:52.483 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.483 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15810[0m
2024-11-29 14:00:52.483 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.492 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.492 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.502 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.502 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:00:52.502 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.502 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.502 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0035, 0.0030, 0.0029]], device='cuda:0')[0m
2024-11-29 14:00:52.503 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  2, 21], device='cuda:0')[0m
2024-11-29 14:00:52.503 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.503 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16350[0m
2024-11-29 14:00:52.503 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.512 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.513 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.522 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.522 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:00:52.522 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.522 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0035, 0.0031]], device='cuda:0')[0m
2024-11-29 14:00:52.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 7], device='cuda:0')[0m
2024-11-29 14:00:52.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15810[0m
2024-11-29 14:00:52.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.533 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.533 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.544 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.544 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:00:52.544 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.544 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.545 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0039, 0.0032]], device='cuda:0')[0m
2024-11-29 14:00:52.545 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:00:52.545 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.545 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15990[0m
2024-11-29 14:00:52.545 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.554 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.555 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.564 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.564 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:00:52.564 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.564 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.565 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0031, 0.0025, 0.0017]], device='cuda:0')[0m
2024-11-29 14:00:52.565 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:00:52.565 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.565 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15990[0m
2024-11-29 14:00:52.566 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.574 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.575 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.586 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.586 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:00:52.586 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.586 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.587 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0036, 0.0035, 0.0026]], device='cuda:0')[0m
2024-11-29 14:00:52.587 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 6], device='cuda:0')[0m
2024-11-29 14:00:52.587 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.587 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15810[0m
2024-11-29 14:00:52.587 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.596 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.596 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.605 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.605 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:00:52.606 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.606 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.606 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0036, 0.0032, 0.0030]], device='cuda:0')[0m
2024-11-29 14:00:52.606 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:00:52.607 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.607 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15990[0m
2024-11-29 14:00:52.607 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.616 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.616 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.625 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.625 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:00:52.625 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.625 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.626 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0044, 0.0038]], device='cuda:0')[0m
2024-11-29 14:00:52.626 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:00:52.626 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.626 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15990[0m
2024-11-29 14:00:52.626 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.635 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.635 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.646 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.646 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:00:52.646 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.646 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.646 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0054, 0.0033]], device='cuda:0')[0m
2024-11-29 14:00:52.647 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([2, 0, 1], device='cuda:0')[0m
2024-11-29 14:00:52.647 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.647 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15990[0m
2024-11-29 14:00:52.647 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:00:52.656 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:00:52.656 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:00:52.667 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:00:52.667 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:00:52.667 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:00:52.667 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1115), (4, 1115, 1835), (5, 1835, 2195), (6, 2195, 2915), (7, 2915, 3095), (8, 3095, 3275), (9, 3275, 3455), (10, 3455, 3635), (11, 3635, 3995), (12, 3995, 4175), (13, 4175, 4535), (14, 4535, 5255), (15, 5255, 5435), (16, 5435, 5795), (17, 5795, 6515), (18, 6515, 6875), (19, 6875, 7055), (20, 7055, 7775), (21, 7775, 8135), (22, 8135, 8855), (23, 8855, 9035), (24, 9035, 9395), (25, 9395, 10115), (26, 10115, 10547)][0m
2024-11-29 14:00:52.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0069, 0.0060, 0.0040]], device='cuda:0')[0m
2024-11-29 14:00:52.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([2, 0, 1], device='cuda:0')[0m
2024-11-29 14:00:52.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10590[0m
2024-11-29 14:00:52.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15990[0m
2024-11-29 14:00:52.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (C) Romance
Pred C) Romance
##########
2222222 C C
11111111111111 C C
Part  Acc: 66.67%
------------------------------ topic_reasoning ------------------------------
  1%|          | 3/264 [01:13<1:48:20, 24.91s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What type of film is this?
Options:
(A) History
(B) Romance
(C) Action
(D) Comedy
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:01:12.117 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.118 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.134 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.134 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:01:12.134 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.134 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0039, 0.0039, 0.0039]], device='cuda:0')[0m
2024-11-29 14:01:12.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25,  8,  4], device='cuda:0')[0m
2024-11-29 14:01:12.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15486[0m
2024-11-29 14:01:12.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.144 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.145 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.153 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.153 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:01:12.153 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.154 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.154 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0060, 0.0058]], device='cuda:0')[0m
2024-11-29 14:01:12.154 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:01:12.154 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.155 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.155 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.163 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.163 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.181 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.181 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:01:12.182 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.182 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.182 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0054, 0.0053]], device='cuda:0')[0m
2024-11-29 14:01:12.183 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 2, 13,  1], device='cuda:0')[0m
2024-11-29 14:01:12.183 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.183 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14730[0m
2024-11-29 14:01:12.183 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.191 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.192 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.221 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.222 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:01:12.222 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.222 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.222 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0070, 0.0067, 0.0061]], device='cuda:0')[0m
2024-11-29 14:01:12.223 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  2, 11], device='cuda:0')[0m
2024-11-29 14:01:12.223 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.223 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14730[0m
2024-11-29 14:01:12.223 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.232 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.232 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.241 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.241 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:01:12.241 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.241 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0040, 0.0037]], device='cuda:0')[0m
2024-11-29 14:01:12.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  2, 11], device='cuda:0')[0m
2024-11-29 14:01:12.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14730[0m
2024-11-29 14:01:12.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.251 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.251 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.266 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.266 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:01:12.266 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.266 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.267 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0044, 0.0037, 0.0036]], device='cuda:0')[0m
2024-11-29 14:01:12.267 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 14,  2], device='cuda:0')[0m
2024-11-29 14:01:12.267 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.267 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14730[0m
2024-11-29 14:01:12.267 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.276 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.276 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.288 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.289 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:01:12.289 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.289 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.289 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0079, 0.0070, 0.0062]], device='cuda:0')[0m
2024-11-29 14:01:12.290 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  2, 14], device='cuda:0')[0m
2024-11-29 14:01:12.290 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.290 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14730[0m
2024-11-29 14:01:12.290 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.299 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.299 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.308 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.308 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:01:12.308 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.308 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0042, 0.0039]], device='cuda:0')[0m
2024-11-29 14:01:12.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 15, 14], device='cuda:0')[0m
2024-11-29 14:01:12.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.318 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.318 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.328 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.328 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:01:12.328 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.328 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.329 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0056, 0.0047, 0.0044]], device='cuda:0')[0m
2024-11-29 14:01:12.329 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 2, 0], device='cuda:0')[0m
2024-11-29 14:01:12.329 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.329 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.329 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.338 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.338 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:01:12.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.365 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0065, 0.0050, 0.0049]], device='cuda:0')[0m
2024-11-29 14:01:12.365 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:01:12.365 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.365 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.365 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.374 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.374 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.383 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.383 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:01:12.383 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.383 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.384 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0065, 0.0057, 0.0049]], device='cuda:0')[0m
2024-11-29 14:01:12.384 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  2, 11], device='cuda:0')[0m
2024-11-29 14:01:12.384 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.384 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14730[0m
2024-11-29 14:01:12.384 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.393 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.393 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.402 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.402 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:01:12.402 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.402 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.403 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0058, 0.0048, 0.0047]], device='cuda:0')[0m
2024-11-29 14:01:12.403 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:01:12.403 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.403 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.403 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.412 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.412 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.424 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.424 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:01:12.424 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.424 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.425 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0047, 0.0040]], device='cuda:0')[0m
2024-11-29 14:01:12.425 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:01:12.425 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.425 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.425 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.435 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.435 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.452 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.452 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:01:12.452 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.452 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0063, 0.0056, 0.0054]], device='cuda:0')[0m
2024-11-29 14:01:12.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:01:12.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.463 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.463 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.472 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.473 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:01:12.473 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.473 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.474 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0046, 0.0041, 0.0041]], device='cuda:0')[0m
2024-11-29 14:01:12.474 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 2, 0], device='cuda:0')[0m
2024-11-29 14:01:12.474 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.474 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.474 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.483 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.483 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.504 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.504 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:01:12.504 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.505 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.505 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0052, 0.0045]], device='cuda:0')[0m
2024-11-29 14:01:12.506 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:01:12.506 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.506 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.506 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.515 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.515 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.540 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.541 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:01:12.541 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.541 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.542 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0033, 0.0031, 0.0029]], device='cuda:0')[0m
2024-11-29 14:01:12.542 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([2, 1, 0], device='cuda:0')[0m
2024-11-29 14:01:12.542 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.542 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.542 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.551 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.551 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.573 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.573 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:01:12.573 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.573 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.574 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0033, 0.0027, 0.0026]], device='cuda:0')[0m
2024-11-29 14:01:12.574 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:12.574 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.574 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.575 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.584 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.584 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.601 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.601 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:01:12.601 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.601 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.602 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0045, 0.0044]], device='cuda:0')[0m
2024-11-29 14:01:12.602 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:12.602 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.602 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.603 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.611 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.612 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:01:12.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0040, 0.0039, 0.0039]], device='cuda:0')[0m
2024-11-29 14:01:12.629 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:01:12.629 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.629 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.629 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.638 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.638 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.651 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.651 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:01:12.651 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.651 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.652 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0044, 0.0042]], device='cuda:0')[0m
2024-11-29 14:01:12.652 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:12.652 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.652 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.652 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.661 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.661 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.671 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.672 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:01:12.672 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.672 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.672 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0055, 0.0050]], device='cuda:0')[0m
2024-11-29 14:01:12.673 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:12.673 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.673 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.673 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.681 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.682 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.719 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.719 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:01:12.720 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.720 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.720 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0037, 0.0030, 0.0027]], device='cuda:0')[0m
2024-11-29 14:01:12.721 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:12.721 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.721 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.721 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.730 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.730 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.746 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.747 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:01:12.747 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.747 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.747 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0042, 0.0040, 0.0036]], device='cuda:0')[0m
2024-11-29 14:01:12.748 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:12.748 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.748 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.748 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.757 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.757 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.767 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.767 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:01:12.767 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.767 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0044, 0.0042, 0.0042]], device='cuda:0')[0m
2024-11-29 14:01:12.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:01:12.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.777 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.777 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.786 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.786 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:01:12.786 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.786 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.787 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0055, 0.0047]], device='cuda:0')[0m
2024-11-29 14:01:12.787 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:12.787 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.787 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.787 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.796 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.796 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.805 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.806 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:01:12.806 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.806 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.806 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0058, 0.0045]], device='cuda:0')[0m
2024-11-29 14:01:12.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:12.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14910[0m
2024-11-29 14:01:12.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:12.815 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:12.815 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:12.825 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:12.825 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:01:12.825 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:12.825 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 755), (4, 755, 1115), (5, 1115, 1835), (6, 1835, 2015), (7, 2015, 2375), (8, 2375, 2555), (9, 2555, 3275), (10, 3275, 3995), (11, 3995, 4175), (12, 4175, 4355), (13, 4355, 4715), (14, 4715, 4895), (15, 4895, 5075), (16, 5075, 5435), (17, 5435, 5615), (18, 5615, 6335), (19, 6335, 7055), (20, 7055, 7235), (21, 7235, 7595), (22, 7595, 8315), (23, 8315, 8495), (24, 8495, 9215), (25, 9215, 9395), (26, 9395, 9827)][0m
2024-11-29 14:01:12.825 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0091, 0.0072, 0.0064]], device='cuda:0')[0m
2024-11-29 14:01:12.826 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([15,  0,  2], device='cuda:0')[0m
2024-11-29 14:01:12.826 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 9870[0m
2024-11-29 14:01:12.826 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15090[0m
2024-11-29 14:01:12.826 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (C) Action
Pred B) Romance
##########
2222222 B C
Part  Acc: 50.00%
------------------------------ topic_reasoning ------------------------------
  2%|▏         | 4/264 [01:33<1:39:48, 23.03s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the genre of this film?
Options:
(A) Sci-Fi
(B) Romance
(C) Action
(D) Mystery
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:01:39.264 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:39.264 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:39.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:39.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:01:39.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:39.547 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:39.548 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0040, 0.0040, 0.0040]], device='cuda:0')[0m
2024-11-29 14:01:39.548 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([13, 12,  6], device='cuda:0')[0m
2024-11-29 14:01:39.548 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:39.548 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16496[0m
2024-11-29 14:01:39.548 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:39.560 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:39.561 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:39.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:39.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:01:39.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:39.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:39.769 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0077, 0.0066, 0.0060]], device='cuda:0')[0m
2024-11-29 14:01:39.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:39.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:39.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:39.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:39.780 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:39.780 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:39.962 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:39.963 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:01:39.963 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:39.963 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:39.964 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0070, 0.0064, 0.0059]], device='cuda:0')[0m
2024-11-29 14:01:39.964 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:39.964 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:39.964 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:39.964 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:39.973 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:39.974 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:40.175 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:40.176 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:01:40.176 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:40.176 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:40.177 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0098, 0.0077, 0.0072]], device='cuda:0')[0m
2024-11-29 14:01:40.177 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:40.177 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:40.177 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:40.177 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:40.187 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:40.187 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:40.393 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:40.394 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:01:40.394 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:40.394 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:40.395 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0061, 0.0046, 0.0046]], device='cuda:0')[0m
2024-11-29 14:01:40.395 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:40.395 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:40.395 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:40.395 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:40.407 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:40.408 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:40.618 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:40.619 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:01:40.619 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:40.619 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:40.620 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0070, 0.0044, 0.0042]], device='cuda:0')[0m
2024-11-29 14:01:40.620 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:40.620 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:40.620 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:40.620 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:40.629 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:40.630 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:40.702 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:40.703 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:01:40.703 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:40.703 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:40.704 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0099, 0.0081, 0.0080]], device='cuda:0')[0m
2024-11-29 14:01:40.704 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:40.704 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:40.704 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:40.704 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:40.713 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:40.714 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:40.929 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:40.929 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:01:40.930 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:40.930 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:40.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0044, 0.0042]], device='cuda:0')[0m
2024-11-29 14:01:40.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:40.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:40.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:40.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:40.941 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:40.941 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:41.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:41.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:01:41.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:41.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:41.158 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0083, 0.0052, 0.0052]], device='cuda:0')[0m
2024-11-29 14:01:41.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:41.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:41.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:41.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:41.168 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:41.169 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:41.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:41.379 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:01:41.379 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:41.379 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:41.380 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0093, 0.0055, 0.0047]], device='cuda:0')[0m
2024-11-29 14:01:41.380 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:41.380 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:41.380 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:41.380 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:41.390 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:41.390 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:41.582 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:41.583 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:01:41.583 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:41.583 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:41.584 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0073, 0.0057, 0.0048]], device='cuda:0')[0m
2024-11-29 14:01:41.584 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:41.584 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:41.584 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:41.584 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:41.594 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:41.594 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:41.801 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:41.801 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:01:41.801 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:41.801 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:41.802 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0078, 0.0049, 0.0049]], device='cuda:0')[0m
2024-11-29 14:01:41.803 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:41.803 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:41.803 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:41.803 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:41.813 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:41.813 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:42.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:42.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:01:42.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:42.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:42.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0061, 0.0042, 0.0035]], device='cuda:0')[0m
2024-11-29 14:01:42.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:42.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:42.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:42.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:42.048 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:42.048 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:42.235 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:42.235 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:01:42.235 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:42.235 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:42.236 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0070, 0.0070, 0.0051]], device='cuda:0')[0m
2024-11-29 14:01:42.237 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([2, 0, 1], device='cuda:0')[0m
2024-11-29 14:01:42.237 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:42.237 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:42.237 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:42.246 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:42.246 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:42.441 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:42.441 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:01:42.441 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:42.441 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:42.442 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0052, 0.0041, 0.0040]], device='cuda:0')[0m
2024-11-29 14:01:42.443 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:42.443 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:42.443 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:42.443 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:42.452 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:42.453 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:42.667 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:42.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:01:42.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:42.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:42.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0066, 0.0054, 0.0052]], device='cuda:0')[0m
2024-11-29 14:01:42.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:42.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:42.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:42.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:42.679 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:42.679 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:42.892 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:42.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:01:42.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:42.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:42.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0038, 0.0036, 0.0032]], device='cuda:0')[0m
2024-11-29 14:01:42.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([2, 0, 1], device='cuda:0')[0m
2024-11-29 14:01:42.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:42.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:42.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:42.909 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:42.910 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:43.110 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:43.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:01:43.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:43.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:43.117 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0036, 0.0031, 0.0029]], device='cuda:0')[0m
2024-11-29 14:01:43.117 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  2, 22], device='cuda:0')[0m
2024-11-29 14:01:43.117 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:43.117 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15416[0m
2024-11-29 14:01:43.117 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:43.127 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:43.128 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:43.361 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:43.361 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:01:43.361 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:43.361 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:43.362 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0049, 0.0046]], device='cuda:0')[0m
2024-11-29 14:01:43.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:43.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:43.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:43.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:43.372 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:43.373 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:43.561 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:43.561 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:01:43.561 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:43.561 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:43.563 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0061, 0.0041, 0.0040]], device='cuda:0')[0m
2024-11-29 14:01:43.563 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:43.563 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:43.563 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:43.563 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:43.573 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:43.574 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:43.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:43.768 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:01:43.769 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:43.769 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:43.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0065, 0.0042, 0.0038]], device='cuda:0')[0m
2024-11-29 14:01:43.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:43.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:43.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:43.770 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:43.780 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:43.780 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:43.966 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:43.966 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:01:43.966 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:43.966 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:43.967 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0081, 0.0052, 0.0047]], device='cuda:0')[0m
2024-11-29 14:01:43.968 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:43.968 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:43.968 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:43.968 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:43.978 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:43.978 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:44.192 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:44.192 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:01:44.192 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:44.192 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:44.193 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0029, 0.0028]], device='cuda:0')[0m
2024-11-29 14:01:44.194 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:44.194 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:44.194 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:44.194 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:44.203 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:44.203 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:44.394 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:44.397 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:01:44.397 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:44.398 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:44.398 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0056, 0.0044, 0.0041]], device='cuda:0')[0m
2024-11-29 14:01:44.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:44.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:44.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:44.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:44.408 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:44.409 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:44.626 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:44.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:01:44.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:44.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:44.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0059, 0.0041, 0.0037]], device='cuda:0')[0m
2024-11-29 14:01:44.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:44.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:44.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:44.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:44.638 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:44.638 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:44.817 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:44.818 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:01:44.818 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:44.818 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:44.819 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0070, 0.0062, 0.0053]], device='cuda:0')[0m
2024-11-29 14:01:44.823 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:44.823 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:44.823 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:44.824 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:44.833 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:44.833 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:45.058 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:45.059 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:01:45.059 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:45.059 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:45.060 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0078, 0.0063, 0.0063]], device='cuda:0')[0m
2024-11-29 14:01:45.060 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:01:45.060 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:45.060 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:45.060 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:01:45.071 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:01:45.071 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:01:45.297 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:01:45.297 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:01:45.297 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:01:45.297 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 395), (3, 395, 575), (4, 575, 935), (5, 935, 1295), (6, 1295, 1475), (7, 1475, 2195), (8, 2195, 2375), (9, 2375, 2735), (10, 2735, 3095), (11, 3095, 3455), (12, 3455, 3815), (13, 3815, 4535), (14, 4535, 5255), (15, 5255, 5975), (16, 5975, 6695), (17, 6695, 7055), (18, 7055, 7235), (19, 7235, 7595), (20, 7595, 7955), (21, 7955, 8135), (22, 8135, 8495), (23, 8495, 9215), (24, 9215, 9395), (25, 9395, 9755), (26, 9755, 9971)][0m
2024-11-29 14:01:45.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0094, 0.0086, 0.0077]], device='cuda:0')[0m
2024-11-29 14:01:45.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:01:45.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10016[0m
2024-11-29 14:01:45.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14876[0m
2024-11-29 14:01:45.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (C) Action
Pred C) Action
##########
2222222 C C
11111111111111 C C
Part  Acc: 60.00%
------------------------------ topic_reasoning ------------------------------
  2%|▏         | 5/264 [02:06<1:54:16, 26.47s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the first-person character doing in this video?
Options:
(A) Making coffee
(B) Making milk
(C) Making a cake
(D) Baking cookies
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:02:15.588 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:15.589 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:15.861 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:15.862 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:02:15.862 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:15.862 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:15.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0041, 0.0041, 0.0041]], device='cuda:0')[0m
2024-11-29 14:02:15.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([7, 5, 2], device='cuda:0')[0m
2024-11-29 14:02:15.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:15.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 18124[0m
2024-11-29 14:02:15.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:15.874 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:15.875 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:16.072 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:16.072 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:02:16.072 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:16.072 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:16.073 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0065, 0.0055, 0.0054]], device='cuda:0')[0m
2024-11-29 14:02:16.074 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 4], device='cuda:0')[0m
2024-11-29 14:02:16.074 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:16.074 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16864[0m
2024-11-29 14:02:16.074 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:16.085 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:16.086 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:16.234 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:16.235 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:02:16.235 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:16.235 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:16.235 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0060, 0.0059]], device='cuda:0')[0m
2024-11-29 14:02:16.236 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([15, 14,  4], device='cuda:0')[0m
2024-11-29 14:02:16.236 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:16.236 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16504[0m
2024-11-29 14:02:16.236 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:16.247 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:16.247 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:16.451 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:16.451 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:02:16.451 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:16.451 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:16.452 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0071, 0.0068, 0.0067]], device='cuda:0')[0m
2024-11-29 14:02:16.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4, 10, 20], device='cuda:0')[0m
2024-11-29 14:02:16.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:16.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16504[0m
2024-11-29 14:02:16.453 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:16.463 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:16.464 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:16.629 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:16.630 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:02:16.630 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:16.630 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:16.631 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0043, 0.0043]], device='cuda:0')[0m
2024-11-29 14:02:16.632 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4, 20, 10], device='cuda:0')[0m
2024-11-29 14:02:16.632 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:16.632 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16504[0m
2024-11-29 14:02:16.632 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:16.643 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:16.643 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:16.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:16.852 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:02:16.852 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:16.852 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:16.856 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0038, 0.0037]], device='cuda:0')[0m
2024-11-29 14:02:16.857 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4, 14, 10], device='cuda:0')[0m
2024-11-29 14:02:16.857 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:16.857 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16504[0m
2024-11-29 14:02:16.857 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:16.868 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:16.868 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:17.084 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:17.094 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:02:17.094 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:17.094 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:17.095 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0079, 0.0068, 0.0067]], device='cuda:0')[0m
2024-11-29 14:02:17.095 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4,  0, 10], device='cuda:0')[0m
2024-11-29 14:02:17.095 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:17.095 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:17.095 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:17.106 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:17.106 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:17.341 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:17.341 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:02:17.341 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:17.341 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:17.342 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0039, 0.0038]], device='cuda:0')[0m
2024-11-29 14:02:17.343 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  4, 20], device='cuda:0')[0m
2024-11-29 14:02:17.343 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:17.343 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:17.343 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:17.354 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:17.355 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:17.569 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:17.569 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:02:17.569 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:17.569 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:17.570 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0052, 0.0047, 0.0037]], device='cuda:0')[0m
2024-11-29 14:02:17.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([4, 0, 1], device='cuda:0')[0m
2024-11-29 14:02:17.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:17.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16864[0m
2024-11-29 14:02:17.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:17.582 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:17.582 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:17.805 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:17.805 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:02:17.805 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:17.805 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:17.807 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0049, 0.0048, 0.0040]], device='cuda:0')[0m
2024-11-29 14:02:17.811 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  4, 20], device='cuda:0')[0m
2024-11-29 14:02:17.811 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:17.811 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:17.811 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:17.823 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:17.823 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:18.049 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:18.050 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:02:18.050 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:18.050 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:18.051 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0048, 0.0047]], device='cuda:0')[0m
2024-11-29 14:02:18.051 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4, 10, 20], device='cuda:0')[0m
2024-11-29 14:02:18.051 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:18.052 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16504[0m
2024-11-29 14:02:18.052 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:18.063 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:18.063 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:18.283 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:18.284 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:02:18.284 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:18.284 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:18.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0049, 0.0047, 0.0040]], device='cuda:0')[0m
2024-11-29 14:02:18.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4,  0, 14], device='cuda:0')[0m
2024-11-29 14:02:18.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:18.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:18.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:18.297 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:18.297 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:18.503 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:18.503 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:02:18.503 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:18.503 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:18.504 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0041, 0.0041, 0.0033]], device='cuda:0')[0m
2024-11-29 14:02:18.504 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  4, 14], device='cuda:0')[0m
2024-11-29 14:02:18.505 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:18.505 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:18.505 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:18.516 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:18.516 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:18.686 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:18.686 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:02:18.686 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:18.686 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:18.687 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0051, 0.0047]], device='cuda:0')[0m
2024-11-29 14:02:18.688 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4,  0, 22], device='cuda:0')[0m
2024-11-29 14:02:18.688 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:18.688 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:18.688 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:18.699 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:18.699 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:18.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:18.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:02:18.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:18.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:18.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0057, 0.0041, 0.0041]], device='cuda:0')[0m
2024-11-29 14:02:18.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4, 25,  0], device='cuda:0')[0m
2024-11-29 14:02:18.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:18.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16144[0m
2024-11-29 14:02:18.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:18.915 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:18.916 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:19.149 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:19.150 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:02:19.150 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:19.150 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:19.151 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0062, 0.0052, 0.0050]], device='cuda:0')[0m
2024-11-29 14:02:19.151 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4,  0, 22], device='cuda:0')[0m
2024-11-29 14:02:19.151 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:19.151 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:19.151 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:19.162 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:19.163 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:19.369 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:19.370 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:02:19.370 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:19.370 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:19.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0062, 0.0046, 0.0039]], device='cuda:0')[0m
2024-11-29 14:02:19.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4, 25, 24], device='cuda:0')[0m
2024-11-29 14:02:19.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:19.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16144[0m
2024-11-29 14:02:19.371 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:19.383 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:19.383 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:19.633 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:19.634 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:02:19.634 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:19.634 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:19.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0046, 0.0042, 0.0041]], device='cuda:0')[0m
2024-11-29 14:02:19.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25, 22,  4], device='cuda:0')[0m
2024-11-29 14:02:19.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:19.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15964[0m
2024-11-29 14:02:19.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:19.647 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:19.647 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:19.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:19.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:02:19.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:19.863 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:19.864 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0054, 0.0045, 0.0045]], device='cuda:0')[0m
2024-11-29 14:02:19.864 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4, 22,  0], device='cuda:0')[0m
2024-11-29 14:02:19.864 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:19.864 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:19.865 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:19.876 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:19.876 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:20.123 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:20.124 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:02:20.124 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:20.124 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:20.125 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0048, 0.0042]], device='cuda:0')[0m
2024-11-29 14:02:20.125 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4,  0, 25], device='cuda:0')[0m
2024-11-29 14:02:20.125 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:20.125 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16144[0m
2024-11-29 14:02:20.125 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:20.137 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:20.138 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:20.375 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:20.375 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:02:20.375 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:20.375 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:20.376 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0053, 0.0052, 0.0044]], device='cuda:0')[0m
2024-11-29 14:02:20.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 4,  0, 25], device='cuda:0')[0m
2024-11-29 14:02:20.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:20.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16144[0m
2024-11-29 14:02:20.378 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:20.389 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:20.390 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:20.610 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:20.615 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:02:20.615 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:20.616 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:20.617 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0059, 0.0044]], device='cuda:0')[0m
2024-11-29 14:02:20.617 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([4, 0, 1], device='cuda:0')[0m
2024-11-29 14:02:20.618 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:20.618 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16864[0m
2024-11-29 14:02:20.618 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:20.630 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:20.630 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:20.835 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:20.835 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:02:20.835 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:20.835 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:20.836 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0039, 0.0031, 0.0027]], device='cuda:0')[0m
2024-11-29 14:02:20.837 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 1], device='cuda:0')[0m
2024-11-29 14:02:20.837 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:20.837 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16864[0m
2024-11-29 14:02:20.837 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:20.848 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:20.848 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:20.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:20.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:02:20.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:20.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:20.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0039, 0.0034]], device='cuda:0')[0m
2024-11-29 14:02:20.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([4, 0, 6], device='cuda:0')[0m
2024-11-29 14:02:20.902 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:20.902 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16864[0m
2024-11-29 14:02:20.902 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:20.912 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:20.913 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:21.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:21.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:02:21.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:21.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:21.136 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0043, 0.0034]], device='cuda:0')[0m
2024-11-29 14:02:21.137 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  4, 10], device='cuda:0')[0m
2024-11-29 14:02:21.137 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:21.137 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16684[0m
2024-11-29 14:02:21.137 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:21.148 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:21.148 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:21.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:21.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:02:21.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:21.363 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:21.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0058, 0.0054, 0.0044]], device='cuda:0')[0m
2024-11-29 14:02:21.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 6], device='cuda:0')[0m
2024-11-29 14:02:21.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:21.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16864[0m
2024-11-29 14:02:21.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:21.375 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:21.376 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:21.596 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:21.597 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:02:21.597 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:21.597 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:21.598 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0058, 0.0057, 0.0044]], device='cuda:0')[0m
2024-11-29 14:02:21.598 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([4, 0, 6], device='cuda:0')[0m
2024-11-29 14:02:21.598 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:21.598 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16864[0m
2024-11-29 14:02:21.598 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:21.610 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:21.610 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:21.839 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:21.839 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:02:21.839 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:21.839 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 755), (3, 755, 1475), (4, 1475, 1835), (5, 1835, 2015), (6, 2015, 2735), (7, 2735, 3095), (8, 3095, 3815), (9, 3815, 4175), (10, 4175, 4535), (11, 4535, 4715), (12, 4715, 5435), (13, 5435, 6155), (14, 6155, 6515), (15, 6515, 6695), (16, 6695, 6875), (17, 6875, 7595), (18, 7595, 8315), (19, 8315, 8495), (20, 8495, 9215), (21, 9215, 9395), (22, 9395, 10115), (23, 10115, 10295), (24, 10295, 11015), (25, 11015, 11375), (26, 11375, 11591)][0m
2024-11-29 14:02:21.840 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0058, 0.0053, 0.0050]], device='cuda:0')[0m
2024-11-29 14:02:21.840 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([4, 0, 6], device='cuda:0')[0m
2024-11-29 14:02:21.840 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11644[0m
2024-11-29 14:02:21.841 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16864[0m
2024-11-29 14:02:21.841 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (A) Making coffee
Pred A) Making coffee
##########
2222222 A A
11111111111111 A A
Part  Acc: 66.67%
------------------------------ topic_reasoning ------------------------------
  2%|▏         | 6/264 [02:43<2:08:42, 29.93s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What story does the whole video tell?
Options:
(A) Criminal Investigation
(B) Wedding Scene
(C) Drama Performance
(D) Chase Incident
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:02:47.015 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:47.016 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:47.155 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:47.156 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:02:47.156 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:47.156 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:47.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0042, 0.0042, 0.0042]], device='cuda:0')[0m
2024-11-29 14:02:47.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25, 10,  0], device='cuda:0')[0m
2024-11-29 14:02:47.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:47.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16031[0m
2024-11-29 14:02:47.157 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:47.167 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:47.167 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:47.314 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:47.314 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:02:47.314 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:47.314 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:47.315 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0054, 0.0050]], device='cuda:0')[0m
2024-11-29 14:02:47.315 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 4, 2], device='cuda:0')[0m
2024-11-29 14:02:47.315 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:47.315 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15455[0m
2024-11-29 14:02:47.315 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:47.324 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:47.325 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:47.476 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:47.476 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:02:47.476 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:47.476 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:47.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0056, 0.0053, 0.0053]], device='cuda:0')[0m
2024-11-29 14:02:47.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 3, 22, 16], device='cuda:0')[0m
2024-11-29 14:02:47.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:47.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:47.477 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:47.487 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:47.487 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:47.633 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:47.634 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:02:47.634 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:47.634 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:47.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0070, 0.0069, 0.0061]], device='cuda:0')[0m
2024-11-29 14:02:47.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 4, 9], device='cuda:0')[0m
2024-11-29 14:02:47.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:47.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:47.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:47.645 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:47.645 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:47.789 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:47.790 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:02:47.790 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:47.790 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:47.791 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0042, 0.0041, 0.0037]], device='cuda:0')[0m
2024-11-29 14:02:47.791 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([4, 3, 9], device='cuda:0')[0m
2024-11-29 14:02:47.791 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:47.791 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:47.791 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:47.800 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:47.801 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:47.952 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:47.953 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:02:47.953 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:47.953 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:47.954 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0041, 0.0038]], device='cuda:0')[0m
2024-11-29 14:02:47.954 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 9, 4], device='cuda:0')[0m
2024-11-29 14:02:47.954 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:47.954 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:47.954 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:47.963 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:47.964 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:48.115 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:48.115 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:02:48.115 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:48.115 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:48.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0073, 0.0071, 0.0068]], device='cuda:0')[0m
2024-11-29 14:02:48.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 4, 9], device='cuda:0')[0m
2024-11-29 14:02:48.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:48.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:48.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:48.126 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:48.126 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:48.276 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:48.276 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:02:48.276 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:48.276 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:48.277 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0039, 0.0039, 0.0039]], device='cuda:0')[0m
2024-11-29 14:02:48.277 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([9, 3, 6], device='cuda:0')[0m
2024-11-29 14:02:48.278 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:48.278 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15455[0m
2024-11-29 14:02:48.278 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:48.287 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:48.287 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:48.436 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:48.437 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:02:48.437 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:48.437 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:48.438 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0040, 0.0036]], device='cuda:0')[0m
2024-11-29 14:02:48.438 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 9, 6], device='cuda:0')[0m
2024-11-29 14:02:48.438 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:48.438 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15455[0m
2024-11-29 14:02:48.438 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:48.448 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:48.448 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:48.600 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:48.600 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:02:48.600 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:48.600 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:48.601 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0043, 0.0043]], device='cuda:0')[0m
2024-11-29 14:02:48.602 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 3, 16,  9], device='cuda:0')[0m
2024-11-29 14:02:48.602 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:48.602 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:48.602 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:48.612 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:48.613 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:48.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:48.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:02:48.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:48.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:48.765 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0049, 0.0049, 0.0045]], device='cuda:0')[0m
2024-11-29 14:02:48.766 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 9, 4], device='cuda:0')[0m
2024-11-29 14:02:48.766 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:48.766 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:48.766 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:48.775 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:48.775 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:48.926 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:48.926 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:02:48.927 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:48.927 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:48.927 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0044, 0.0040, 0.0036]], device='cuda:0')[0m
2024-11-29 14:02:48.928 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 4, 9], device='cuda:0')[0m
2024-11-29 14:02:48.928 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:48.928 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:48.928 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:48.938 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:48.938 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:49.086 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:49.086 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:02:49.086 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:49.086 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:49.087 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0057, 0.0041, 0.0033]], device='cuda:0')[0m
2024-11-29 14:02:49.087 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:49.087 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:49.088 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:49.088 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:49.097 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:49.098 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:49.249 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:49.249 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:02:49.249 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:49.249 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:49.250 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0058, 0.0051]], device='cuda:0')[0m
2024-11-29 14:02:49.250 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:49.250 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:49.250 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:49.250 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:49.260 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:49.260 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:49.406 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:49.406 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:02:49.406 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:49.406 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:49.407 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0086, 0.0059, 0.0049]], device='cuda:0')[0m
2024-11-29 14:02:49.408 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:49.408 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:49.408 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:49.408 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:49.418 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:49.418 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:49.569 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:49.570 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:02:49.570 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:49.570 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:49.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0090, 0.0056, 0.0055]], device='cuda:0')[0m
2024-11-29 14:02:49.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:49.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:49.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:49.571 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:49.581 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:49.582 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:49.731 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:49.731 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:02:49.731 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:49.732 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:49.732 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0099, 0.0058, 0.0052]], device='cuda:0')[0m
2024-11-29 14:02:49.733 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:49.733 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:49.733 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:49.733 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:49.743 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:49.743 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:49.894 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:49.895 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:02:49.895 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:49.895 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:49.896 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0104, 0.0043, 0.0037]], device='cuda:0')[0m
2024-11-29 14:02:49.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:49.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:49.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:49.897 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:49.907 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:49.908 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:50.055 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:50.056 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:02:50.056 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:50.056 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:50.057 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0061, 0.0060, 0.0054]], device='cuda:0')[0m
2024-11-29 14:02:50.057 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:50.057 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:50.057 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:50.057 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:50.067 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:50.067 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:50.133 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:50.134 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:02:50.134 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:50.134 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:50.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0081, 0.0048, 0.0044]], device='cuda:0')[0m
2024-11-29 14:02:50.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:50.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:50.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:50.135 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:50.145 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:50.146 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:50.301 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:50.301 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:02:50.301 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:50.301 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:50.302 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0074, 0.0048, 0.0042]], device='cuda:0')[0m
2024-11-29 14:02:50.303 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([22,  3,  4], device='cuda:0')[0m
2024-11-29 14:02:50.303 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:50.303 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:50.303 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:50.313 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:50.313 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:50.459 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:50.459 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:02:50.459 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:50.459 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:50.460 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0057, 0.0047, 0.0038]], device='cuda:0')[0m
2024-11-29 14:02:50.460 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 4, 2], device='cuda:0')[0m
2024-11-29 14:02:50.461 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:50.461 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15455[0m
2024-11-29 14:02:50.461 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:50.470 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:50.471 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:50.623 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:50.623 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:02:50.623 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:50.623 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:50.624 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0034, 0.0022, 0.0022]], device='cuda:0')[0m
2024-11-29 14:02:50.625 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 4, 2], device='cuda:0')[0m
2024-11-29 14:02:50.625 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:50.625 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15455[0m
2024-11-29 14:02:50.625 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:50.634 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:50.635 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:50.781 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:50.781 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:02:50.782 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:50.782 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:50.783 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0051, 0.0034, 0.0034]], device='cuda:0')[0m
2024-11-29 14:02:50.783 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 9, 4], device='cuda:0')[0m
2024-11-29 14:02:50.783 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:50.783 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:50.783 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:50.793 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:50.793 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:50.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:50.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:02:50.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:50.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:50.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0046, 0.0035, 0.0034]], device='cuda:0')[0m
2024-11-29 14:02:50.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 4, 9], device='cuda:0')[0m
2024-11-29 14:02:50.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:50.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:50.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:50.957 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:50.957 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:51.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:51.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:02:51.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:51.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:51.106 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0052, 0.0040, 0.0040]], device='cuda:0')[0m
2024-11-29 14:02:51.107 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 9, 4], device='cuda:0')[0m
2024-11-29 14:02:51.107 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:51.107 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15275[0m
2024-11-29 14:02:51.107 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:51.117 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:51.117 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:51.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:51.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:02:51.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:51.309 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:51.310 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0050, 0.0047, 0.0040]], device='cuda:0')[0m
2024-11-29 14:02:51.311 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 0, 1], device='cuda:0')[0m
2024-11-29 14:02:51.311 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:51.311 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15995[0m
2024-11-29 14:02:51.311 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:02:51.321 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:02:51.322 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:02:51.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:02:51.524 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:02:51.524 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:02:51.524 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 1115), (3, 1115, 1475), (4, 1475, 1655), (5, 1655, 1835), (6, 1835, 2195), (7, 2195, 2555), (8, 2555, 2915), (9, 2915, 3275), (10, 3275, 3455), (11, 3455, 4175), (12, 4175, 4895), (13, 4895, 5255), (14, 5255, 5975), (15, 5975, 6695), (16, 6695, 7415), (17, 7415, 7595), (18, 7595, 7775), (19, 7775, 7955), (20, 7955, 8315), (21, 8315, 8495), (22, 8495, 9215), (23, 9215, 9395), (24, 9395, 9755), (25, 9755, 9935), (26, 9935, 10367)][0m
2024-11-29 14:02:51.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0050, 0.0049]], device='cuda:0')[0m
2024-11-29 14:02:51.526 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([3, 0, 2], device='cuda:0')[0m
2024-11-29 14:02:51.526 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10415[0m
2024-11-29 14:02:51.526 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15995[0m
2024-11-29 14:02:51.526 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (A) Criminal Investigation
Pred A) Criminal Investigation
##########
2222222 A A
11111111111111 A A
Part  Acc: 71.43%
------------------------------ topic_reasoning ------------------------------
  3%|▎         | 7/264 [03:12<2:07:48, 29.84s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What season is it in the video?
Options:
(A) Summer
(B) Spring
(C) Autumn
(D) Winter
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:03:16.960 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:16.961 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:16.970 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:16.970 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:03:16.970 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:16.971 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:16.971 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0039, 0.0039, 0.0039]], device='cuda:0')[0m
2024-11-29 14:03:16.971 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([7, 6, 4], device='cuda:0')[0m
2024-11-29 14:03:16.971 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:16.972 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16567[0m
2024-11-29 14:03:16.972 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:16.981 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:16.982 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:16.991 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:16.991 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:03:16.991 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:16.991 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:16.991 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0058, 0.0049]], device='cuda:0')[0m
2024-11-29 14:03:16.992 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:03:16.992 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:16.992 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:16.992 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.001 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.001 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:03:17.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0056, 0.0051, 0.0049]], device='cuda:0')[0m
2024-11-29 14:03:17.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 17, 24], device='cuda:0')[0m
2024-11-29 14:03:17.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14947[0m
2024-11-29 14:03:17.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.046 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.047 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.104 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.104 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:03:17.104 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.104 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0071, 0.0060, 0.0058]], device='cuda:0')[0m
2024-11-29 14:03:17.106 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  8, 12], device='cuda:0')[0m
2024-11-29 14:03:17.106 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.106 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14947[0m
2024-11-29 14:03:17.106 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.115 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.115 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.145 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.145 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:03:17.145 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.145 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.146 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0036, 0.0036]], device='cuda:0')[0m
2024-11-29 14:03:17.146 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 12,  8], device='cuda:0')[0m
2024-11-29 14:03:17.146 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.146 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 14947[0m
2024-11-29 14:03:17.146 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.155 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.155 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.164 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.165 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:03:17.165 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.165 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.165 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0040, 0.0035, 0.0034]], device='cuda:0')[0m
2024-11-29 14:03:17.165 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 12,  0], device='cuda:0')[0m
2024-11-29 14:03:17.166 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.166 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15127[0m
2024-11-29 14:03:17.166 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.174 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.175 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.191 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.191 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:03:17.191 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.191 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.192 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0076, 0.0061, 0.0059]], device='cuda:0')[0m
2024-11-29 14:03:17.193 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 12], device='cuda:0')[0m
2024-11-29 14:03:17.193 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.193 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15127[0m
2024-11-29 14:03:17.193 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.202 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.202 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.240 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.241 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:03:17.241 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.241 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0037, 0.0036, 0.0034]], device='cuda:0')[0m
2024-11-29 14:03:17.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 12], device='cuda:0')[0m
2024-11-29 14:03:17.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15127[0m
2024-11-29 14:03:17.242 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.252 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.252 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.286 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.286 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:03:17.286 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.286 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.287 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0041, 0.0037, 0.0033]], device='cuda:0')[0m
2024-11-29 14:03:17.287 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:17.287 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.288 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:17.288 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.297 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.297 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.320 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.320 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:03:17.321 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.321 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.321 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0046, 0.0038, 0.0037]], device='cuda:0')[0m
2024-11-29 14:03:17.322 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:03:17.322 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.322 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:17.322 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.332 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.332 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.347 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.347 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:03:17.347 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.347 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.348 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0042, 0.0042]], device='cuda:0')[0m
2024-11-29 14:03:17.348 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 8], device='cuda:0')[0m
2024-11-29 14:03:17.348 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.348 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15127[0m
2024-11-29 14:03:17.349 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.358 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.358 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.435 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.435 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:03:17.435 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.435 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.436 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0045, 0.0033]], device='cuda:0')[0m
2024-11-29 14:03:17.437 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:03:17.437 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.437 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:17.437 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.446 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.447 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.609 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.609 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:03:17.609 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.610 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.611 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0038, 0.0032, 0.0031]], device='cuda:0')[0m
2024-11-29 14:03:17.611 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:17.611 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.611 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:17.611 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.621 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.622 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.739 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.740 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:03:17.740 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.740 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.742 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0043, 0.0040]], device='cuda:0')[0m
2024-11-29 14:03:17.742 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:03:17.742 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.742 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:17.742 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.752 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.752 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:17.906 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:17.910 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:03:17.910 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:17.910 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:17.911 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0038, 0.0027]], device='cuda:0')[0m
2024-11-29 14:03:17.911 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:17.911 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:17.911 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:17.911 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:17.921 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:17.921 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:03:18.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.037 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0050, 0.0049, 0.0040]], device='cuda:0')[0m
2024-11-29 14:03:18.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:18.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:18.038 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.048 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.048 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.197 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.198 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:03:18.198 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.198 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0034, 0.0033, 0.0031]], device='cuda:0')[0m
2024-11-29 14:03:18.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 23], device='cuda:0')[0m
2024-11-29 14:03:18.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15667[0m
2024-11-29 14:03:18.199 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.209 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.209 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.344 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.344 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:03:18.344 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.345 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.346 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0033, 0.0028, 0.0026]], device='cuda:0')[0m
2024-11-29 14:03:18.347 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([23,  0,  2], device='cuda:0')[0m
2024-11-29 14:03:18.347 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.347 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15847[0m
2024-11-29 14:03:18.347 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.356 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.357 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.493 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.494 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:03:18.494 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.494 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0043, 0.0040]], device='cuda:0')[0m
2024-11-29 14:03:18.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:18.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:18.496 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.505 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.505 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.562 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.562 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:03:18.563 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.563 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.563 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0053, 0.0041, 0.0028]], device='cuda:0')[0m
2024-11-29 14:03:18.564 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 23], device='cuda:0')[0m
2024-11-29 14:03:18.564 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.564 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15667[0m
2024-11-29 14:03:18.564 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.573 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.573 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.591 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:03:18.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0045, 0.0037]], device='cuda:0')[0m
2024-11-29 14:03:18.593 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:18.593 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.593 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:18.593 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.602 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.602 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.622 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.622 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:03:18.622 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.622 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.623 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0066, 0.0046, 0.0038]], device='cuda:0')[0m
2024-11-29 14:03:18.623 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:18.623 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.624 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:18.624 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.632 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.633 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.751 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.751 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:03:18.751 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.751 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.753 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0041, 0.0028, 0.0023]], device='cuda:0')[0m
2024-11-29 14:03:18.753 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:18.753 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.753 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:18.753 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.765 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.766 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:18.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:18.948 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:03:18.948 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:18.948 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:18.950 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0035, 0.0034]], device='cuda:0')[0m
2024-11-29 14:03:18.950 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-11-29 14:03:18.952 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:18.952 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:18.953 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:18.964 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:18.964 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:19.120 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:19.120 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:03:19.120 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:19.120 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:19.121 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0042, 0.0036, 0.0032]], device='cuda:0')[0m
2024-11-29 14:03:19.122 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:19.122 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:19.122 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:19.122 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:19.132 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:19.132 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:19.315 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:19.320 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:03:19.320 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:19.320 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:19.321 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0056, 0.0050, 0.0045]], device='cuda:0')[0m
2024-11-29 14:03:19.322 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:19.322 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:19.322 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:19.322 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:19.332 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:19.332 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:19.428 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:19.428 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:03:19.428 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:19.428 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:19.432 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0067, 0.0051, 0.0045]], device='cuda:0')[0m
2024-11-29 14:03:19.433 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:03:19.433 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:19.433 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15307[0m
2024-11-29 14:03:19.433 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:19.443 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:19.444 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:19.622 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:19.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:03:19.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:19.627 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3815), (9, 3815, 3995), (10, 3995, 4355), (11, 4355, 4715), (12, 4715, 5075), (13, 5075, 5255), (14, 5255, 5975), (15, 5975, 6335), (16, 6335, 7055), (17, 7055, 7415), (18, 7415, 7595), (19, 7595, 7955), (20, 7955, 8135), (21, 8135, 8495), (22, 8495, 8675), (23, 8675, 9035), (24, 9035, 9755), (25, 9755, 9935), (26, 9935, 10043)][0m
2024-11-29 14:03:19.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0066, 0.0061, 0.0051]], device='cuda:0')[0m
2024-11-29 14:03:19.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([7, 0, 2], device='cuda:0')[0m
2024-11-29 14:03:19.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10087[0m
2024-11-29 14:03:19.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15847[0m
2024-11-29 14:03:19.628 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (D) Winter
Pred D) Winter
##########
2222222 D D
11111111111111 D D
Part  Acc: 75.00%
------------------------------ topic_reasoning ------------------------------
  3%|▎         | 8/264 [03:40<2:04:50, 29.26s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the background of the video?
Options:
(A) Desert
(B) Undersea
(C) Forest
(D) Beach
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:03:38.785 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:38.786 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:39.188 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:39.189 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:03:39.189 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:39.189 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:39.190 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0036, 0.0036, 0.0036]], device='cuda:0')[0m
2024-11-29 14:03:39.190 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([7, 2, 0], device='cuda:0')[0m
2024-11-29 14:03:39.190 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:39.190 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17648[0m
2024-11-29 14:03:39.190 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:39.209 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:39.209 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:39.574 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:39.574 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:03:39.574 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:39.574 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:39.576 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0050, 0.0049]], device='cuda:0')[0m
2024-11-29 14:03:39.581 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 4, 3], device='cuda:0')[0m
2024-11-29 14:03:39.581 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:39.581 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:39.581 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:39.591 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:39.592 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:39.983 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:39.983 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:03:39.983 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:39.984 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:39.984 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0052, 0.0050, 0.0048]], device='cuda:0')[0m
2024-11-29 14:03:39.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  6, 11], device='cuda:0')[0m
2024-11-29 14:03:39.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:39.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:39.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:39.994 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:39.994 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:40.319 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:40.319 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:03:40.319 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:40.319 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:40.320 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0071, 0.0062, 0.0062]], device='cuda:0')[0m
2024-11-29 14:03:40.320 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 4, 6], device='cuda:0')[0m
2024-11-29 14:03:40.320 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:40.321 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:40.321 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:40.330 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:40.331 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:40.634 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:40.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:03:40.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:40.635 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:40.636 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0041, 0.0040]], device='cuda:0')[0m
2024-11-29 14:03:40.636 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 4, 6], device='cuda:0')[0m
2024-11-29 14:03:40.636 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:40.636 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:40.636 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:40.658 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:40.659 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:40.908 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:40.909 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:03:40.909 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:40.909 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:40.910 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0053, 0.0039, 0.0038]], device='cuda:0')[0m
2024-11-29 14:03:40.910 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 6, 4], device='cuda:0')[0m
2024-11-29 14:03:40.910 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:40.910 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:40.911 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:40.921 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:40.921 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:41.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:41.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:03:41.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:41.162 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:41.162 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0075, 0.0068, 0.0066]], device='cuda:0')[0m
2024-11-29 14:03:41.163 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 6, 4], device='cuda:0')[0m
2024-11-29 14:03:41.163 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:41.163 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:41.163 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:41.173 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:41.173 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:41.447 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:41.447 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:03:41.447 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:41.447 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:41.448 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0043, 0.0036, 0.0035]], device='cuda:0')[0m
2024-11-29 14:03:41.449 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 23, 11], device='cuda:0')[0m
2024-11-29 14:03:41.449 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:41.449 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:41.449 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:41.459 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:41.460 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:41.508 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:41.508 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:03:41.508 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:41.509 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:41.509 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0051, 0.0032, 0.0031]], device='cuda:0')[0m
2024-11-29 14:03:41.510 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 3, 6], device='cuda:0')[0m
2024-11-29 14:03:41.510 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:41.510 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:41.510 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:41.520 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:41.520 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:41.723 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:41.723 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:03:41.723 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:41.723 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:41.724 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0050, 0.0036, 0.0034]], device='cuda:0')[0m
2024-11-29 14:03:41.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 11, 23], device='cuda:0')[0m
2024-11-29 14:03:41.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:41.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:41.725 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:41.735 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:41.735 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:41.974 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:41.974 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:03:41.974 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:41.974 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:41.975 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0053, 0.0042, 0.0038]], device='cuda:0')[0m
2024-11-29 14:03:41.975 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 11,  3], device='cuda:0')[0m
2024-11-29 14:03:41.975 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:41.975 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:41.975 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:41.985 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:41.986 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:42.258 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:42.259 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:03:42.259 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:42.259 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:42.260 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0050, 0.0034, 0.0034]], device='cuda:0')[0m
2024-11-29 14:03:42.261 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 11,  3], device='cuda:0')[0m
2024-11-29 14:03:42.261 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:42.261 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:42.261 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:42.272 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:42.272 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:42.493 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:42.494 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:03:42.494 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:42.494 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:42.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0038, 0.0036, 0.0031]], device='cuda:0')[0m
2024-11-29 14:03:42.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 20, 11], device='cuda:0')[0m
2024-11-29 14:03:42.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:42.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16208[0m
2024-11-29 14:03:42.495 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:42.506 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:42.506 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:42.758 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:42.758 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:03:42.758 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:42.758 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:42.759 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0044, 0.0043]], device='cuda:0')[0m
2024-11-29 14:03:42.760 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 20, 11], device='cuda:0')[0m
2024-11-29 14:03:42.760 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:42.760 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16208[0m
2024-11-29 14:03:42.760 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:42.770 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:42.771 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:43.018 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:43.018 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:03:43.018 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:43.018 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:43.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0051, 0.0048, 0.0043]], device='cuda:0')[0m
2024-11-29 14:03:43.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([20,  1,  0], device='cuda:0')[0m
2024-11-29 14:03:43.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:43.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16748[0m
2024-11-29 14:03:43.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:43.031 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:43.031 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:43.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:43.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:03:43.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:43.298 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:43.299 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0059, 0.0055, 0.0043]], device='cuda:0')[0m
2024-11-29 14:03:43.300 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 20, 11], device='cuda:0')[0m
2024-11-29 14:03:43.300 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:43.300 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16208[0m
2024-11-29 14:03:43.300 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:43.310 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:43.311 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:43.519 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:43.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:03:43.524 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:43.524 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:43.524 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0051, 0.0038]], device='cuda:0')[0m
2024-11-29 14:03:43.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([20,  1,  0], device='cuda:0')[0m
2024-11-29 14:03:43.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:43.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16748[0m
2024-11-29 14:03:43.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:43.535 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:43.535 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:43.775 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:43.775 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:03:43.775 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:43.775 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:43.777 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0038, 0.0034]], device='cuda:0')[0m
2024-11-29 14:03:43.777 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([20,  1,  0], device='cuda:0')[0m
2024-11-29 14:03:43.777 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:43.777 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16748[0m
2024-11-29 14:03:43.778 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:43.788 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:43.788 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:44.032 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:44.032 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:03:44.033 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:44.033 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:44.035 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0050, 0.0042, 0.0041]], device='cuda:0')[0m
2024-11-29 14:03:44.035 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 20,  6], device='cuda:0')[0m
2024-11-29 14:03:44.035 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:44.035 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16208[0m
2024-11-29 14:03:44.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:44.046 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:44.047 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:44.283 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:44.283 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:03:44.284 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:44.284 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:44.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0043, 0.0040]], device='cuda:0')[0m
2024-11-29 14:03:44.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([20,  1,  0], device='cuda:0')[0m
2024-11-29 14:03:44.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:44.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16748[0m
2024-11-29 14:03:44.285 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:44.295 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:44.295 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:44.468 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:44.469 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:03:44.469 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:44.469 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:44.470 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0044, 0.0039]], device='cuda:0')[0m
2024-11-29 14:03:44.470 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([20,  1,  0], device='cuda:0')[0m
2024-11-29 14:03:44.470 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:44.470 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16748[0m
2024-11-29 14:03:44.470 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:44.481 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:44.481 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:44.689 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:44.689 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:03:44.689 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:44.689 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:44.690 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0051, 0.0043, 0.0036]], device='cuda:0')[0m
2024-11-29 14:03:44.691 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 11], device='cuda:0')[0m
2024-11-29 14:03:44.691 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:44.691 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16568[0m
2024-11-29 14:03:44.691 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:44.702 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:44.702 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:44.927 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:44.928 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:03:44.928 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:44.928 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:44.929 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0029, 0.0027, 0.0018]], device='cuda:0')[0m
2024-11-29 14:03:44.929 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 6], device='cuda:0')[0m
2024-11-29 14:03:44.929 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:44.934 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16568[0m
2024-11-29 14:03:44.934 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:44.945 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:44.945 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:45.188 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:45.188 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:03:45.189 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:45.189 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:45.190 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0037, 0.0031, 0.0030]], device='cuda:0')[0m
2024-11-29 14:03:45.191 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 6], device='cuda:0')[0m
2024-11-29 14:03:45.191 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:45.191 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16568[0m
2024-11-29 14:03:45.191 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:45.202 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:45.202 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:45.478 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:45.478 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:03:45.478 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:45.478 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:45.479 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0039, 0.0036, 0.0035]], device='cuda:0')[0m
2024-11-29 14:03:45.479 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 11,  6], device='cuda:0')[0m
2024-11-29 14:03:45.480 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:45.480 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16028[0m
2024-11-29 14:03:45.480 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:45.489 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:45.490 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:45.716 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:45.716 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:03:45.716 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:45.716 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:45.717 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0047, 0.0038]], device='cuda:0')[0m
2024-11-29 14:03:45.720 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 3], device='cuda:0')[0m
2024-11-29 14:03:45.720 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:45.720 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16568[0m
2024-11-29 14:03:45.720 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:45.731 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:45.731 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:45.925 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:45.926 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:03:45.926 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:45.926 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:45.927 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0056, 0.0046, 0.0038]], device='cuda:0')[0m
2024-11-29 14:03:45.927 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 3], device='cuda:0')[0m
2024-11-29 14:03:45.927 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:45.927 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16568[0m
2024-11-29 14:03:45.928 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:03:45.942 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:03:45.942 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:03:46.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:03:46.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:03:46.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:03:46.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 755), (2, 755, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2555), (8, 2555, 3275), (9, 3275, 3995), (10, 3995, 4715), (11, 4715, 5075), (12, 5075, 5255), (13, 5255, 5975), (14, 5975, 6335), (15, 6335, 6515), (16, 6515, 7235), (17, 7235, 7415), (18, 7415, 8135), (19, 8135, 8315), (20, 8315, 8675), (21, 8675, 9035), (22, 9035, 9755), (23, 9755, 10115), (24, 10115, 10295), (25, 10295, 11015), (26, 11015, 11123)][0m
2024-11-29 14:03:46.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0056, 0.0044]], device='cuda:0')[0m
2024-11-29 14:03:46.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([11,  0,  1], device='cuda:0')[0m
2024-11-29 14:03:46.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 11168[0m
2024-11-29 14:03:46.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 16568[0m
2024-11-29 14:03:46.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (B) Undersea
Pred B) Undersea
##########
2222222 B B
11111111111111 B B
Part  Acc: 77.78%
------------------------------ topic_reasoning ------------------------------
  3%|▎         | 9/264 [04:07<2:00:56, 28.46s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the main setting of the video?
Options:
(A) Desert
(B) Ocean
(C) City
(D) Palace
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:04:14.460 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:14.460 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:14.756 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:14.756 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:04:14.756 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:14.756 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:14.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0032, 0.0032, 0.0032]], device='cuda:0')[0m
2024-11-29 14:04:14.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([8, 6, 4], device='cuda:0')[0m
2024-11-29 14:04:14.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:14.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 19016[0m
2024-11-29 14:04:14.757 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:14.768 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:14.769 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:14.977 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:14.978 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:04:14.978 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:14.978 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:14.979 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0052, 0.0049, 0.0042]], device='cuda:0')[0m
2024-11-29 14:04:14.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-11-29 14:04:14.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:14.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17756[0m
2024-11-29 14:04:14.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:14.990 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:14.991 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:15.211 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:15.211 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:04:15.211 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:15.211 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:15.212 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0055, 0.0047, 0.0043]], device='cuda:0')[0m
2024-11-29 14:04:15.212 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 14], device='cuda:0')[0m
2024-11-29 14:04:15.212 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:15.212 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:15.212 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:15.223 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:15.223 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:15.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:15.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:04:15.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:15.454 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:15.456 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0075, 0.0055, 0.0053]], device='cuda:0')[0m
2024-11-29 14:04:15.456 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 14], device='cuda:0')[0m
2024-11-29 14:04:15.456 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:15.456 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:15.456 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:15.467 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:15.467 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:15.735 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:15.735 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:04:15.735 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:15.735 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:15.736 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0046, 0.0033, 0.0033]], device='cuda:0')[0m
2024-11-29 14:04:15.737 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1, 12, 14], device='cuda:0')[0m
2024-11-29 14:04:15.737 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:15.737 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17396[0m
2024-11-29 14:04:15.737 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:15.748 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:15.748 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:15.945 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:15.945 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:04:15.945 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:15.945 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:15.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0049, 0.0036, 0.0029]], device='cuda:0')[0m
2024-11-29 14:04:15.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 12], device='cuda:0')[0m
2024-11-29 14:04:15.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:15.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:15.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:15.956 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:15.957 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:16.158 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:16.158 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:04:16.158 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:16.158 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:16.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0079, 0.0059, 0.0053]], device='cuda:0')[0m
2024-11-29 14:04:16.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 14], device='cuda:0')[0m
2024-11-29 14:04:16.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:16.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:16.159 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:16.169 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:16.170 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:16.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:16.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:04:16.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:16.399 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:16.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0040, 0.0038, 0.0029]], device='cuda:0')[0m
2024-11-29 14:04:16.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 12], device='cuda:0')[0m
2024-11-29 14:04:16.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:16.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:16.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:16.411 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:16.412 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:16.656 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:16.656 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:04:16.656 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:16.656 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:16.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0049, 0.0046, 0.0029]], device='cuda:0')[0m
2024-11-29 14:04:16.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 12], device='cuda:0')[0m
2024-11-29 14:04:16.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:16.657 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:16.658 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:16.668 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:16.669 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:16.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:16.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:04:16.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:16.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:16.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0046, 0.0045, 0.0034]], device='cuda:0')[0m
2024-11-29 14:04:16.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 12], device='cuda:0')[0m
2024-11-29 14:04:16.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:16.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:16.900 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:16.911 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:16.911 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:17.128 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:17.128 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:04:17.128 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:17.128 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:17.129 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0057, 0.0043, 0.0038]], device='cuda:0')[0m
2024-11-29 14:04:17.130 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 12], device='cuda:0')[0m
2024-11-29 14:04:17.130 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:17.130 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:17.130 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:17.145 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:17.145 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:17.387 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:17.387 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:04:17.387 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:17.387 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:17.388 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0051, 0.0046, 0.0032]], device='cuda:0')[0m
2024-11-29 14:04:17.388 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 12], device='cuda:0')[0m
2024-11-29 14:04:17.388 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:17.388 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:17.388 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:17.399 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:17.399 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:17.590 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:17.590 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:04:17.590 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:17.590 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:17.591 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0046, 0.0039]], device='cuda:0')[0m
2024-11-29 14:04:17.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 25], device='cuda:0')[0m
2024-11-29 14:04:17.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:17.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17036[0m
2024-11-29 14:04:17.592 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:17.602 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:17.602 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:17.793 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:17.793 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:04:17.793 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:17.793 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:17.794 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0061, 0.0056, 0.0048]], device='cuda:0')[0m
2024-11-29 14:04:17.794 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 1,  0, 25], device='cuda:0')[0m
2024-11-29 14:04:17.794 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:17.794 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17036[0m
2024-11-29 14:04:17.794 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:17.805 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:17.805 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:18.017 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:18.017 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:04:18.017 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:18.017 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:18.018 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0057, 0.0056, 0.0052]], device='cuda:0')[0m
2024-11-29 14:04:18.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25,  0,  1], device='cuda:0')[0m
2024-11-29 14:04:18.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:18.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17036[0m
2024-11-29 14:04:18.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:18.029 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:18.030 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:18.269 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:18.269 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:04:18.269 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:18.269 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:18.270 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0064, 0.0062, 0.0057]], device='cuda:0')[0m
2024-11-29 14:04:18.275 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 25], device='cuda:0')[0m
2024-11-29 14:04:18.275 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:18.275 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17036[0m
2024-11-29 14:04:18.275 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:18.286 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:18.286 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:18.519 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:18.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:04:18.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:18.523 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:18.524 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0058, 0.0053, 0.0053]], device='cuda:0')[0m
2024-11-29 14:04:18.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25,  1,  0], device='cuda:0')[0m
2024-11-29 14:04:18.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:18.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17036[0m
2024-11-29 14:04:18.525 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:18.535 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:18.536 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:18.776 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:18.776 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:04:18.776 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:18.776 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:18.777 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0059, 0.0054, 0.0037]], device='cuda:0')[0m
2024-11-29 14:04:18.777 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25,  0,  1], device='cuda:0')[0m
2024-11-29 14:04:18.778 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:18.778 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17036[0m
2024-11-29 14:04:18.778 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:18.788 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:18.789 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:18.983 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:18.984 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:04:18.984 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:18.984 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:18.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0062, 0.0056, 0.0044]], device='cuda:0')[0m
2024-11-29 14:04:18.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 12], device='cuda:0')[0m
2024-11-29 14:04:18.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:18.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:18.985 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:18.997 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:18.997 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:19.229 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:19.229 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:04:19.229 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:19.229 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:19.230 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0063, 0.0052, 0.0048]], device='cuda:0')[0m
2024-11-29 14:04:19.231 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0, 25,  1], device='cuda:0')[0m
2024-11-29 14:04:19.231 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:19.231 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17036[0m
2024-11-29 14:04:19.231 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:19.242 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:19.242 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:19.465 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:19.465 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:04:19.465 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:19.465 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:19.466 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0063, 0.0050, 0.0049]], device='cuda:0')[0m
2024-11-29 14:04:19.469 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 25], device='cuda:0')[0m
2024-11-29 14:04:19.469 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:19.470 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17036[0m
2024-11-29 14:04:19.470 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:19.480 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:19.481 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:19.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:19.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:04:19.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:19.680 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:19.681 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0072, 0.0054, 0.0040]], device='cuda:0')[0m
2024-11-29 14:04:19.682 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 12], device='cuda:0')[0m
2024-11-29 14:04:19.682 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:19.682 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:19.682 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:19.692 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:19.693 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:19.903 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:19.903 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:04:19.903 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:19.903 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:19.904 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0044, 0.0033, 0.0019]], device='cuda:0')[0m
2024-11-29 14:04:19.904 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:04:19.905 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:19.905 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17756[0m
2024-11-29 14:04:19.905 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:19.915 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:19.916 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:20.155 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:20.155 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-11-29 14:04:20.155 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:20.155 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:20.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0050, 0.0041, 0.0029]], device='cuda:0')[0m
2024-11-29 14:04:20.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 12], device='cuda:0')[0m
2024-11-29 14:04:20.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:20.160 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:20.161 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:20.171 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:20.171 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:20.414 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:20.414 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-11-29 14:04:20.414 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:20.414 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:20.415 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0042, 0.0033]], device='cuda:0')[0m
2024-11-29 14:04:20.416 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 12], device='cuda:0')[0m
2024-11-29 14:04:20.416 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:20.416 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17576[0m
2024-11-29 14:04:20.416 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:20.432 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:20.433 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:20.667 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:20.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-11-29 14:04:20.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:20.668 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:20.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0063, 0.0055, 0.0039]], device='cuda:0')[0m
2024-11-29 14:04:20.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:04:20.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:20.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17756[0m
2024-11-29 14:04:20.669 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:20.680 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:20.680 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:20.871 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:20.871 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-11-29 14:04:20.871 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:20.871 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:20.872 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0077, 0.0052, 0.0037]], device='cuda:0')[0m
2024-11-29 14:04:20.872 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:04:20.873 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:20.873 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17756[0m
2024-11-29 14:04:20.873 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:20.883 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:20.884 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:21.104 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:21.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-11-29 14:04:21.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:21.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 395), (2, 395, 575), (3, 575, 935), (4, 935, 1295), (5, 1295, 2015), (6, 2015, 2375), (7, 2375, 3095), (8, 3095, 3455), (9, 3455, 4175), (10, 4175, 4535), (11, 4535, 4895), (12, 4895, 5615), (13, 5615, 5795), (14, 5795, 6155), (15, 6155, 6335), (16, 6335, 7055), (17, 7055, 7775), (18, 7775, 8135), (19, 8135, 8855), (20, 8855, 9575), (21, 9575, 9935), (22, 9935, 10655), (23, 10655, 10835), (24, 10835, 11555), (25, 11555, 12275), (26, 12275, 12491)][0m
2024-11-29 14:04:21.106 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0083, 0.0058, 0.0048]], device='cuda:0')[0m
2024-11-29 14:04:21.110 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-11-29 14:04:21.111 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 12536[0m
2024-11-29 14:04:21.111 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17756[0m
2024-11-29 14:04:21.111 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
##########
GT (D) Palace
Pred D) Palace
##########
2222222 D D
11111111111111 D D
Part  Acc: 80.00%
------------------------------ topic_reasoning ------------------------------
  4%|▍         | 10/264 [04:42<2:08:48, 30.43s/it]##### <|im_start|>system
Carefully watch this video and pay attention to every detail. Based on your observations, select the best option that accurately addresses the question.<|im_end|>
<|im_start|>user
<image>
Question: What is the setting of the scene in the video?
Options:
(A) City
(B) Island
(C) Snowy Mountain
(D) Forest
Only give the best option.<|im_end|>
<|im_start|>assistant
Best Option: (
num_images 256
2024-11-29 14:04:49.600 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:49.600 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:50.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:50.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-11-29 14:04:50.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:50.021 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:50.022 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0040, 0.0040, 0.0040]], device='cuda:0')[0m
2024-11-29 14:04:50.022 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([10,  2,  1], device='cuda:0')[0m
2024-11-29 14:04:50.022 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:50.022 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 17472[0m
2024-11-29 14:04:50.022 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:50.033 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:50.033 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:50.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:50.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-11-29 14:04:50.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:50.400 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:50.401 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0078, 0.0052, 0.0050]], device='cuda:0')[0m
2024-11-29 14:04:50.401 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 3, 4], device='cuda:0')[0m
2024-11-29 14:04:50.401 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:50.402 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:50.402 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:50.420 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:50.420 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:50.726 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:50.727 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-11-29 14:04:50.727 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:50.727 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:50.728 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0072, 0.0052, 0.0052]], device='cuda:0')[0m
2024-11-29 14:04:50.728 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  3, 13], device='cuda:0')[0m
2024-11-29 14:04:50.728 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:50.728 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:50.728 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:50.738 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:50.743 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:51.053 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:51.053 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-11-29 14:04:51.053 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:51.053 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:51.054 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0093, 0.0071, 0.0069]], device='cuda:0')[0m
2024-11-29 14:04:51.055 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 3, 4], device='cuda:0')[0m
2024-11-29 14:04:51.055 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:51.055 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:51.055 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:51.068 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:51.069 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:51.373 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:51.373 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-11-29 14:04:51.373 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:51.374 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:51.375 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0046, 0.0045]], device='cuda:0')[0m
2024-11-29 14:04:51.375 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:51.375 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:51.376 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:51.376 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:51.386 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:51.386 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:51.731 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:51.732 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-11-29 14:04:51.732 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:51.732 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:51.733 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0068, 0.0049, 0.0044]], device='cuda:0')[0m
2024-11-29 14:04:51.738 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:51.738 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:51.738 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:51.738 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:51.753 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:51.753 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:52.105 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:52.114 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-11-29 14:04:52.114 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:52.115 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:52.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0093, 0.0081, 0.0075]], device='cuda:0')[0m
2024-11-29 14:04:52.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:52.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:52.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:52.116 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:52.130 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:52.131 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:52.497 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:52.497 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-11-29 14:04:52.497 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:52.497 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:52.498 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0063, 0.0044, 0.0040]], device='cuda:0')[0m
2024-11-29 14:04:52.499 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  4, 25], device='cuda:0')[0m
2024-11-29 14:04:52.499 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:52.499 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15204[0m
2024-11-29 14:04:52.499 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:52.516 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:52.517 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:52.887 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:52.887 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-11-29 14:04:52.887 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:52.887 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:52.888 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0077, 0.0047, 0.0045]], device='cuda:0')[0m
2024-11-29 14:04:52.888 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:52.888 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:52.888 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:52.889 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:52.903 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:52.903 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:53.261 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:53.261 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-11-29 14:04:53.261 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:53.261 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:53.262 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0090, 0.0057, 0.0049]], device='cuda:0')[0m
2024-11-29 14:04:53.262 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:53.263 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:53.263 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:53.263 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:53.273 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:53.276 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:53.610 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:53.614 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-11-29 14:04:53.614 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:53.614 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:53.615 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0077, 0.0060, 0.0056]], device='cuda:0')[0m
2024-11-29 14:04:53.616 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:53.616 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:53.616 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:53.616 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:53.625 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:53.626 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:53.977 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:53.978 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-11-29 14:04:53.978 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:53.978 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:53.979 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0082, 0.0052, 0.0046]], device='cuda:0')[0m
2024-11-29 14:04:53.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:53.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:53.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:53.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:53.990 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:53.995 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:54.360 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:54.360 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-11-29 14:04:54.360 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:54.360 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:54.361 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0085, 0.0047, 0.0037]], device='cuda:0')[0m
2024-11-29 14:04:54.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:54.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:54.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:54.364 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:54.387 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:54.391 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:54.763 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:54.763 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-11-29 14:04:54.763 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:54.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:54.764 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0087, 0.0065, 0.0050]], device='cuda:0')[0m
2024-11-29 14:04:54.773 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:54.773 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:54.773 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:54.773 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:54.783 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:54.784 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:55.126 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:55.126 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-11-29 14:04:55.126 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:55.126 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:55.127 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0084, 0.0056, 0.0045]], device='cuda:0')[0m
2024-11-29 14:04:55.128 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:55.128 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:55.128 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:55.128 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:55.139 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:55.143 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:55.510 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:55.510 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-11-29 14:04:55.510 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:55.511 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:55.511 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0090, 0.0066, 0.0055]], device='cuda:0')[0m
2024-11-29 14:04:55.512 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:55.512 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:55.512 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:55.512 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:55.536 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:55.537 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:55.887 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:55.887 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-11-29 14:04:55.887 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:55.887 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:55.888 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0067, 0.0061, 0.0054]], device='cuda:0')[0m
2024-11-29 14:04:55.892 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:55.892 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:55.892 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:55.893 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:55.907 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:55.908 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:56.233 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:56.237 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-11-29 14:04:56.237 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:56.238 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:56.238 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0070, 0.0041, 0.0034]], device='cuda:0')[0m
2024-11-29 14:04:56.239 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:56.239 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:56.239 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:56.239 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:56.249 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:56.249 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:56.588 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:56.588 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-11-29 14:04:56.588 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:56.588 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:56.589 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0086, 0.0063, 0.0053]], device='cuda:0')[0m
2024-11-29 14:04:56.589 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:56.590 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:56.590 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:56.590 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:56.600 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:56.600 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:56.936 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:56.940 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-11-29 14:04:56.940 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:56.941 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:56.941 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0099, 0.0048, 0.0041]], device='cuda:0')[0m
2024-11-29 14:04:56.942 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:56.942 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:56.942 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:56.942 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:56.959 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:56.959 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:57.316 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:57.316 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-11-29 14:04:57.316 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:57.316 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:57.317 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0109, 0.0051, 0.0039]], device='cuda:0')[0m
2024-11-29 14:04:57.326 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:57.326 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:57.327 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:57.327 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:57.344 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:57.345 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:57.706 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:57.707 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-11-29 14:04:57.707 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:57.707 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:57.708 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0125, 0.0055, 0.0050]], device='cuda:0')[0m
2024-11-29 14:04:57.708 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:57.708 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:57.708 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:57.708 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:57.719 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:57.719 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
2024-11-29 14:04:57.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-11-29 14:04:57.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-11-29 14:04:57.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-11-29 14:04:57.981 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 35, 215), (2, 215, 935), (3, 935, 1655), (4, 1655, 1835), (5, 1835, 2015), (6, 2015, 2375), (7, 2375, 2735), (8, 2735, 3095), (9, 3095, 3455), (10, 3455, 3815), (11, 3815, 4535), (12, 4535, 5255), (13, 5255, 5975), (14, 5975, 6155), (15, 6155, 6515), (16, 6515, 6875), (17, 6875, 7235), (18, 7235, 7415), (19, 7415, 8135), (20, 8135, 8855), (21, 8855, 9575), (22, 9575, 9935), (23, 9935, 10115), (24, 10115, 10475), (25, 10475, 10835), (26, 10835, 10943)][0m
2024-11-29 14:04:57.981 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0081, 0.0037, 0.0032]], device='cuda:0')[0m
2024-11-29 14:04:57.982 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 4, 3], device='cuda:0')[0m
2024-11-29 14:04:57.982 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: 10992[0m
2024-11-29 14:04:57.982 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: 15852[0m
2024-11-29 14:04:57.982 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2024-11-29 14:04:57.992 | INFO     | videoxl.model.language_model.llava_qwen:forward:959 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-11-29 14:04:57.992 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])
