2024-12-01 21:34:08.460 | INFO     | __main__:<module>:16 - This is an INFO message
You are using a model of type qwen2 to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share/minghao/Envs/videoxl/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
_attn_implementation: sdpa
Loading checkpoint shards:   0%|                                                               | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|                                         | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  50%|                           | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  75%|             | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.27s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.43s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
 reload 
 top_k=3
0  q k v states
2024-12-01 21:34:58.779 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.779 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.833 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.833 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 0[0m
2024-12-01 21:34:58.833 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.833 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.834 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0048, 0.0048]], device='cuda:0')[0m
2024-12-01 21:34:58.834 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([25,  1,  0], device='cuda:0')[0m
2024-12-01 21:34:58.834 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.834 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.834 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
1  q k v states
2024-12-01 21:34:58.837 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.838 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 1[0m
2024-12-01 21:34:58.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.842 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0072, 0.0064, 0.0059]], device='cuda:0')[0m
2024-12-01 21:34:58.843 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.843 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.843 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.843 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
2  q k v states
2024-12-01 21:34:58.846 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.846 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.850 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.850 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 2[0m
2024-12-01 21:34:58.850 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.850 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0053, 0.0049]], device='cuda:0')[0m
2024-12-01 21:34:58.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.851 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
3  q k v states
2024-12-01 21:34:58.854 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.854 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.858 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.858 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 3[0m
2024-12-01 21:34:58.858 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.858 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.859 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0067, 0.0058, 0.0051]], device='cuda:0')[0m
2024-12-01 21:34:58.859 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.859 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.859 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.859 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
4  q k v states
2024-12-01 21:34:58.862 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.862 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.866 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.866 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 4[0m
2024-12-01 21:34:58.866 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.866 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.867 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0045, 0.0037, 0.0034]], device='cuda:0')[0m
2024-12-01 21:34:58.867 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 11], device='cuda:0')[0m
2024-12-01 21:34:58.867 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.867 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.867 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
5  q k v states
2024-12-01 21:34:58.870 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.870 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.874 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.874 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 5[0m
2024-12-01 21:34:58.874 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.874 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.875 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0048, 0.0034, 0.0028]], device='cuda:0')[0m
2024-12-01 21:34:58.875 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 11], device='cuda:0')[0m
2024-12-01 21:34:58.875 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.875 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.875 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
6  q k v states
2024-12-01 21:34:58.878 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.878 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.882 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.882 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 6[0m
2024-12-01 21:34:58.882 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.882 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.883 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0068, 0.0059, 0.0052]], device='cuda:0')[0m
2024-12-01 21:34:58.883 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.883 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.883 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.883 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
7  q k v states
2024-12-01 21:34:58.886 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.886 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.890 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.890 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 7[0m
2024-12-01 21:34:58.890 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.890 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.891 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0059, 0.0043, 0.0037]], device='cuda:0')[0m
2024-12-01 21:34:58.891 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 11], device='cuda:0')[0m
2024-12-01 21:34:58.891 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.891 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.891 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
8  q k v states
2024-12-01 21:34:58.894 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.894 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 8[0m
2024-12-01 21:34:58.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.898 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0060, 0.0041, 0.0035]], device='cuda:0')[0m
2024-12-01 21:34:58.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.899 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
9  q k v states
2024-12-01 21:34:58.902 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.902 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.906 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.906 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 9[0m
2024-12-01 21:34:58.906 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.906 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.907 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0069, 0.0046, 0.0041]], device='cuda:0')[0m
2024-12-01 21:34:58.907 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 11], device='cuda:0')[0m
2024-12-01 21:34:58.907 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.907 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.907 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
10  q k v states
2024-12-01 21:34:58.910 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.910 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.914 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.914 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 10[0m
2024-12-01 21:34:58.914 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.914 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.915 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0066, 0.0051, 0.0046]], device='cuda:0')[0m
2024-12-01 21:34:58.915 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.915 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.915 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.915 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
11  q k v states
2024-12-01 21:34:58.918 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.918 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.922 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.922 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 11[0m
2024-12-01 21:34:58.922 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.922 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.923 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0065, 0.0044, 0.0037]], device='cuda:0')[0m
2024-12-01 21:34:58.923 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.923 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.923 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.924 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
12  q k v states
2024-12-01 21:34:58.926 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.927 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.930 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.930 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 12[0m
2024-12-01 21:34:58.930 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0062, 0.0038, 0.0031]], device='cuda:0')[0m
2024-12-01 21:34:58.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.931 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.932 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.932 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
13  q k v states
2024-12-01 21:34:58.934 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.935 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.938 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.938 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 13[0m
2024-12-01 21:34:58.938 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.939 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.939 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0059, 0.0046, 0.0045]], device='cuda:0')[0m
2024-12-01 21:34:58.939 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0,  1, 11], device='cuda:0')[0m
2024-12-01 21:34:58.939 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.940 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.940 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
14  q k v states
2024-12-01 21:34:58.942 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.943 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 14[0m
2024-12-01 21:34:58.946 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0030, 0.0029]], device='cuda:0')[0m
2024-12-01 21:34:58.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.947 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.948 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.948 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
15  q k v states
2024-12-01 21:34:58.950 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.951 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.954 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.954 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 15[0m
2024-12-01 21:34:58.954 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.955 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.955 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0059, 0.0049, 0.0042]], device='cuda:0')[0m
2024-12-01 21:34:58.955 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.956 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.956 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.956 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
16  q k v states
2024-12-01 21:34:58.958 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.959 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.962 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.963 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 16[0m
2024-12-01 21:34:58.963 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.963 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.963 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0037, 0.0035, 0.0032]], device='cuda:0')[0m
2024-12-01 21:34:58.963 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 2, 11,  0], device='cuda:0')[0m
2024-12-01 21:34:58.964 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.964 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.964 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
17  q k v states
2024-12-01 21:34:58.966 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.967 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.970 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.971 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 17[0m
2024-12-01 21:34:58.971 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.971 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.971 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0033, 0.0027, 0.0025]], device='cuda:0')[0m
2024-12-01 21:34:58.972 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0, 11,  1], device='cuda:0')[0m
2024-12-01 21:34:58.972 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.972 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.972 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
18  q k v states
2024-12-01 21:34:58.974 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.975 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.979 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.979 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 18[0m
2024-12-01 21:34:58.979 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.979 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.979 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0050, 0.0045, 0.0043]], device='cuda:0')[0m
2024-12-01 21:34:58.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0, 11,  2], device='cuda:0')[0m
2024-12-01 21:34:58.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.980 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
19  q k v states
2024-12-01 21:34:58.982 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.983 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.987 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.987 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 19[0m
2024-12-01 21:34:58.987 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.987 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.987 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0041, 0.0039, 0.0035]], device='cuda:0')[0m
2024-12-01 21:34:58.988 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([11,  0,  2], device='cuda:0')[0m
2024-12-01 21:34:58.988 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.988 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.988 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
20  q k v states
2024-12-01 21:34:58.991 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.991 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:58.995 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:58.995 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 20[0m
2024-12-01 21:34:58.995 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:58.995 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:58.995 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0049, 0.0037, 0.0036]], device='cuda:0')[0m
2024-12-01 21:34:58.996 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:58.996 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:58.996 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:58.996 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
21  q k v states
2024-12-01 21:34:58.999 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:58.999 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:59.003 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:59.003 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 21[0m
2024-12-01 21:34:59.003 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:59.003 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:59.003 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0052, 0.0047, 0.0043]], device='cuda:0')[0m
2024-12-01 21:34:59.004 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0, 11,  2], device='cuda:0')[0m
2024-12-01 21:34:59.004 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:59.004 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:59.004 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
22  q k v states
2024-12-01 21:34:59.007 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:59.007 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:59.011 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:59.011 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 22[0m
2024-12-01 21:34:59.011 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:59.011 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:59.012 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0026, 0.0025, 0.0023]], device='cuda:0')[0m
2024-12-01 21:34:59.012 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0, 11,  2], device='cuda:0')[0m
2024-12-01 21:34:59.012 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:59.012 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:59.012 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
23  q k v states
2024-12-01 21:34:59.015 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:59.015 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:59.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:59.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 23[0m
2024-12-01 21:34:59.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:59.019 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:59.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0047, 0.0035, 0.0033]], device='cuda:0')[0m
2024-12-01 21:34:59.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([11,  0,  2], device='cuda:0')[0m
2024-12-01 21:34:59.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:59.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:59.020 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
24  q k v states
2024-12-01 21:34:59.023 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:59.023 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:59.027 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:59.027 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 24[0m
2024-12-01 21:34:59.027 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:59.027 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:59.028 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0049, 0.0046, 0.0036]], device='cuda:0')[0m
2024-12-01 21:34:59.028 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([ 0, 11,  2], device='cuda:0')[0m
2024-12-01 21:34:59.028 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:59.028 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:59.028 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
25  q k v states
2024-12-01 21:34:59.031 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:59.031 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:59.035 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:59.035 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 25[0m
2024-12-01 21:34:59.035 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:59.035 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:59.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0059, 0.0053, 0.0049]], device='cuda:0')[0m
2024-12-01 21:34:59.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 1, 2], device='cuda:0')[0m
2024-12-01 21:34:59.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:59.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:59.036 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
26  q k v states
2024-12-01 21:34:59.039 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:59.039 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:59.043 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:59.043 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 26[0m
2024-12-01 21:34:59.043 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:59.043 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:59.044 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0056, 0.0047, 0.0045]], device='cuda:0')[0m
2024-12-01 21:34:59.044 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([0, 2, 1], device='cuda:0')[0m
2024-12-01 21:34:59.044 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:59.044 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:59.044 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
27  q k v states
2024-12-01 21:34:59.047 | INFO     | videoxl.model.language_model.llava_qwen:forward:960 - position_ids_for_attn_score: tensor([[0, 0, 0,  ..., 0, 0, 0]])
2024-12-01 21:34:59.047 | INFO     | videoxl.model.language_model.llava_qwen:forward:961 - position_ids_for_attn_score[:50]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0]])

2024-12-01 21:34:59.051 | INFO     | videoxl.model.language_model.llava_qwen:print_info:819 - [92m====================================================================================================[0m
2024-12-01 21:34:59.051 | INFO     | videoxl.model.language_model.llava_qwen:print_info:829 - [92mLayer_Idx: 27[0m
2024-12-01 21:34:59.051 | INFO     | videoxl.model.language_model.llava_qwen:print_info:830 - [92mTotal Chunks Num: 26[0m
2024-12-01 21:34:59.051 | INFO     | videoxl.model.language_model.llava_qwen:print_info:831 - [92mChunks: [(1, 14, 194), (2, 194, 374), (3, 374, 554), (4, 554, 734), (5, 734, 914), (6, 914, 1094), (7, 1094, 1274), (8, 1274, 1454), (9, 1454, 1634), (10, 1634, 1814), (11, 1814, 1994), (12, 1994, 2174), (13, 2174, 2354), (14, 2354, 2534), (15, 2534, 2714), (16, 2714, 2894), (17, 2894, 3074), (18, 3074, 3254), (19, 3254, 3434), (20, 3434, 3614), (21, 3614, 3794), (22, 3794, 3974), (23, 3974, 4154), (24, 4154, 4334), (25, 4334, 4514), (26, 4514, 4622)][0m
2024-12-01 21:34:59.052 | INFO     | videoxl.model.language_model.llava_qwen:print_info:832 - [92mtopk_values: tensor([[0.0057, 0.0055, 0.0049]], device='cuda:0')[0m
2024-12-01 21:34:59.052 | INFO     | videoxl.model.language_model.llava_qwen:print_info:833 - [92mtopk_indices: tensor([1, 0, 2], device='cuda:0')[0m
2024-12-01 21:34:59.052 | INFO     | videoxl.model.language_model.llava_qwen:print_info:834 - [92moriginal_kv_seq_length: None[0m
2024-12-01 21:34:59.052 | INFO     | videoxl.model.language_model.llava_qwen:print_info:835 - [92mnow_kv_seq_length: None[0m
2024-12-01 21:34:59.052 | INFO     | videoxl.model.language_model.llava_qwen:print_info:837 - [92m====================================================================================================[0m
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
Yes, there is an advertisement in the video. It shows a wristwatch with a black band and a white face against a red background.
